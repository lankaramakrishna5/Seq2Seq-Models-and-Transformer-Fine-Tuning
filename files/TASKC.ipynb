{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-13T14:55:46.961383Z",
     "iopub.status.busy": "2025-04-13T14:55:46.960879Z",
     "iopub.status.idle": "2025-04-13T14:55:47.419725Z",
     "shell.execute_reply": "2025-04-13T14:55:47.419032Z",
     "shell.execute_reply.started": "2025-04-13T14:55:46.961360Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "model_name = \"google-t5/t5-small\"\n",
    "\n",
    "# Load the model\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# Optionally, load the corresponding tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "print(\"Model and tokenizer loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T14:55:47.421314Z",
     "iopub.status.busy": "2025-04-13T14:55:47.420893Z",
     "iopub.status.idle": "2025-04-13T14:55:47.550475Z",
     "shell.execute_reply": "2025-04-13T14:55:47.549723Z",
     "shell.execute_reply.started": "2025-04-13T14:55:47.421294Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loaded: 13879 examples\n",
      "Training set size: 13379\n",
      "Validation set size: 500\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import os\n",
    "\n",
    "# Update this to your actual file path\n",
    "data_path = \"/kaggle/input/assignment2nlp/train.csv\"  # Example: \"/kaggle/input/wiki-data/train.csv\"\n",
    "\n",
    "# Check if the file exists\n",
    "if not os.path.exists(data_path):\n",
    "    raise FileNotFoundError(f\"File not found at: {data_path}\")\n",
    "\n",
    "# Load the dataset from the full path\n",
    "dataset = load_dataset(\"csv\", data_files={\"train\": data_path})[\"train\"]\n",
    "\n",
    "# Split off 500 examples for validation\n",
    "split_dataset = dataset.train_test_split(test_size=500, seed=42)\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "validation_dataset = split_dataset[\"test\"]\n",
    "\n",
    "# Print stats\n",
    "print(f\"Total loaded: {len(dataset)} examples\")\n",
    "print(f\"Training set size: {len(train_dataset)}\")\n",
    "print(f\"Validation set size: {len(validation_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T15:06:48.678847Z",
     "iopub.status.busy": "2025-04-13T15:06:48.678068Z",
     "iopub.status.idle": "2025-04-13T15:06:49.168432Z",
     "shell.execute_reply": "2025-04-13T15:06:49.167657Z",
     "shell.execute_reply.started": "2025-04-13T15:06:48.678817Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports successful.\n",
      "Using model: google-t5/t5-small\n",
      "Tokenizer and model loaded successfully.\n",
      "Train and validation datasets are ready.\n",
      "Tokenizing datasets...\n",
      "Tokenization finished.\n",
      "Columns in tokenized_train: ['input_ids', 'attention_mask', 'labels']\n",
      "Data collator defined.\n",
      "Training arguments defined (wandb disabled).\n",
      "Seq2SeqTrainer initialized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_75/247858808.py:140: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset, DatasetDict # Make sure Dataset, DatasetDict are imported if using them directly\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    Seq2SeqTrainer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    ")\n",
    "import nltk # Needed for rouge calculation later, but good to import early\n",
    "import numpy as np\n",
    "\n",
    "# Ensure NLTK's punkt tokenizer is available (needed for ROUGE)\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except nltk.downloader.DownloadError:\n",
    "    nltk.download('punkt', quiet=True)\n",
    "\n",
    "print(\"Imports successful.\")\n",
    "\n",
    "# --- 1. Define Model Name ---\n",
    "model_name = \"google-t5/t5-small\"\n",
    "print(f\"Using model: {model_name}\")\n",
    "\n",
    "# --- 2. Load Tokenizer and Model ---\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "    print(\"Tokenizer and model loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model or tokenizer: {e}\")\n",
    "    # Handle error appropriately, maybe raise it again if critical\n",
    "    raise\n",
    "\n",
    "# --- 3. Load and Prepare Datasets (ASSUMES train_dataset and validation_dataset exist) ---\n",
    "# Make sure these variables hold your datasets from the previous split\n",
    "# Example of how they might have been created:\n",
    "#\n",
    "# data_path_train = \"/kaggle/input/assignment2nlp/train.csv\"\n",
    "# data_path_test = \"/kaggle/input/assignment2nlp/test.csv\" # Define test path as well\n",
    "#\n",
    "# if not os.path.exists(data_path_train):\n",
    "#     raise FileNotFoundError(f\"Train file not found at: {data_path_train}\")\n",
    "# if not os.path.exists(data_path_test):\n",
    "#      raise FileNotFoundError(f\"Test file not found at: {data_path_test}\")\n",
    "#\n",
    "# Load the dataset from the full path\n",
    "# full_dataset = load_dataset(\"csv\", data_files={\"train\": data_path_train})[\"train\"]\n",
    "# test_data_raw = load_dataset(\"csv\", data_files={\"test\": data_path_test})[\"test\"] # Load test set here too\n",
    "#\n",
    "# Split off 500 examples for validation\n",
    "# split_dataset = full_dataset.train_test_split(test_size=500, seed=42)\n",
    "# train_dataset = split_dataset[\"train\"]\n",
    "# validation_dataset = split_dataset[\"test\"]\n",
    "#\n",
    "# print(f\"Training set size: {len(train_dataset)}\")\n",
    "# print(f\"Validation set size: {len(validation_dataset)}\")\n",
    "# print(f\"Test set size: {len(test_data_raw)}\") # Print test set size\n",
    "\n",
    "# --- Check if datasets exist (replace with your actual variable names) ---\n",
    "if 'train_dataset' not in locals() or 'validation_dataset' not in locals():\n",
    "     raise NameError(\"Variables 'train_dataset' and 'validation_dataset' are not defined. Please run the data loading and splitting code first.\")\n",
    "print(\"Train and validation datasets are ready.\")\n",
    "\n",
    "\n",
    "# --- 4. Define Preprocessing/Tokenization Function ---\n",
    "def tokenize_function(examples):\n",
    "    # Ensure 'text' and 'title' columns exist and handle potential None values\n",
    "    texts = [str(t) if t is not None else \"\" for t in examples['text']]\n",
    "    titles = [str(t) if t is not None else \"\" for t in examples['title']]\n",
    "\n",
    "    # Add prefix for T5 models (optional but recommended for summarization/title generation)\n",
    "    prefix = \"summarize: \" # Or \"generate title: \"\n",
    "    texts = [prefix + text for text in texts]\n",
    "\n",
    "    # Tokenize inputs\n",
    "    model_inputs = tokenizer(\n",
    "        texts, # Use prefixed texts\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\" # Pad to max_length during tokenization\n",
    "    )\n",
    "\n",
    "    # Tokenize targets using text_target argument\n",
    "    labels = tokenizer(\n",
    "        text_target=titles,\n",
    "        max_length=64, # Max length for titles\n",
    "        truncation=True,\n",
    "        padding=\"max_length\" # Pad to max_length during tokenization\n",
    "    )\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# --- 5. Apply Tokenization ---\n",
    "print(\"Tokenizing datasets...\")\n",
    "try:\n",
    "    tokenized_train = train_dataset.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        remove_columns=train_dataset.column_names # Remove original text/title columns\n",
    "    )\n",
    "    tokenized_validation = validation_dataset.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        remove_columns=validation_dataset.column_names # Remove original text/title columns\n",
    "    )\n",
    "    print(\"Tokenization finished.\")\n",
    "    print(f\"Columns in tokenized_train: {tokenized_train.column_names}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during tokenization: {e}\")\n",
    "    raise\n",
    "\n",
    "# --- 6. Define Data Collator ---\n",
    "# Pads sequences dynamically to the longest sequence in a batch\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "print(\"Data collator defined.\")\n",
    "\n",
    "# --- Define Training Arguments ---\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./results_c1_t5_small\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=3,\n",
    "    predict_with_generate=True,\n",
    "    save_steps=500,\n",
    "    eval_steps=500,\n",
    "    logging_steps=100,\n",
    "    fp16=False,\n",
    "    report_to=\"none\" # <--- ADD THIS LINE\n",
    ")\n",
    "print(\"Training arguments defined (wandb disabled).\")\n",
    "\n",
    "# --- 8. Initialize Trainer ---\n",
    "# We'll add compute_metrics later for evaluation\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_validation,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    # compute_metrics=compute_metrics, # We will define and add this for evaluation in the next step\n",
    ")\n",
    "print(\"Seq2SeqTrainer initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T15:07:06.502563Z",
     "iopub.status.busy": "2025-04-13T15:07:06.501932Z",
     "iopub.status.idle": "2025-04-13T15:25:31.532018Z",
     "shell.execute_reply": "2025-04-13T15:25:31.531203Z",
     "shell.execute_reply.started": "2025-04-13T15:07:06.502533Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10035' max='10035' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10035/10035 18:24, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.963200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.142400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.081100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.080100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.064700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.058500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.052400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.047500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.056100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.037900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.048400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.047100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.042100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.035400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.046200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.042700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.044300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.040200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.039000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.039800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.041600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.034200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.035600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.037900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.042500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.038000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.040200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.041500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.031900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.039800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.037000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.032300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.030300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.033800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.034200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.027800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.030900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.029300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>0.038200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.031700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>0.031700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.035300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>0.027500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.036700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.036600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.033000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>0.029700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.031200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>0.028200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.029800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>0.033900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>0.032900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>0.027700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>0.031200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.027900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>0.027400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>0.028700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>0.032300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5900</td>\n",
       "      <td>0.033000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.030000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6100</td>\n",
       "      <td>0.031400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>0.025000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6300</td>\n",
       "      <td>0.028900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>0.030100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.025900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>0.024500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6700</td>\n",
       "      <td>0.028100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>0.028800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6900</td>\n",
       "      <td>0.031000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.027100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7100</td>\n",
       "      <td>0.029000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>0.030300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7300</td>\n",
       "      <td>0.025200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>0.028300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.034100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>0.024600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7700</td>\n",
       "      <td>0.027500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>0.029000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7900</td>\n",
       "      <td>0.025800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.030400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8100</td>\n",
       "      <td>0.030000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>0.026500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8300</td>\n",
       "      <td>0.023700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>0.025600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.023500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8600</td>\n",
       "      <td>0.026000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8700</td>\n",
       "      <td>0.027600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8800</td>\n",
       "      <td>0.023400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8900</td>\n",
       "      <td>0.026300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.022600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9100</td>\n",
       "      <td>0.027000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9200</td>\n",
       "      <td>0.026300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9300</td>\n",
       "      <td>0.023500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9400</td>\n",
       "      <td>0.029100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.021100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9600</td>\n",
       "      <td>0.029800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9700</td>\n",
       "      <td>0.026400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9800</td>\n",
       "      <td>0.025100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9900</td>\n",
       "      <td>0.029400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.024600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished successfully.\n",
      "An error occurred during training: 'TrainOutput' object has no attribute 'metricsz'\n"
     ]
    }
   ],
   "source": [
    "# --- 9. Start Training ---\n",
    "print(\"Starting training...\")\n",
    "try:\n",
    "    train_result = trainer.train()\n",
    "    print(\"Training finished successfully.\")\n",
    "\n",
    "    # Optional: Save metrics and final model\n",
    "    metrics = train_result.metricsz\n",
    "    trainer.log_metrics(\"train\", metrics)\n",
    "    trainer.save_metrics(\"train\", metrics)\n",
    "    trainer.save_state()\n",
    "    # Save the final model\n",
    "    # trainer.save_model(\"./results_c1_t5_small/final_model\")\n",
    "    # print(\"Final model saved.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during training: {e}\")\n",
    "    # You might want to add more specific error handling or logging here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T15:28:21.398004Z",
     "iopub.status.busy": "2025-04-13T15:28:21.397665Z",
     "iopub.status.idle": "2025-04-13T15:28:38.897669Z",
     "shell.execute_reply": "2025-04-13T15:28:38.897076Z",
     "shell.execute_reply.started": "2025-04-13T15:28:21.397976Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test data from: /kaggle/input/assignment2nlp/test.csv\n",
      "Loaded 100 test examples.\n",
      "\n",
      "--- Generating predictions (Greedy Search) ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a64815e87a6e4511b71c50748d46ce0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating (beams=1):   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greedy generation finished in 6.90 seconds.\n",
      "Example Greedy Predictions:\n",
      "  Ref: Weyburn\n",
      "  Gen: Weyburn, Saskatchewan\n",
      "\n",
      "  Ref: Catholic High School, Singapore\n",
      "  Gen: Catholic High School\n",
      "\n",
      "  Ref: Minnesota Golden Gophers\n",
      "  Gen: Minnesota Golden Gophers\n",
      "\n",
      "\n",
      "--- Generating predictions (Beam Search, Num Beams = 4) ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12e65db2dd984a20a254ae93670b5d2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating (beams=4):   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beam search generation finished in 9.84 seconds.\n",
      "Example Beam Search Predictions:\n",
      "  Ref: Weyburn\n",
      "  Gen: Weyburn, Saskatchewan\n",
      "\n",
      "  Ref: Catholic High School, Singapore\n",
      "  Gen: Catholic High School\n",
      "\n",
      "  Ref: Minnesota Golden Gophers\n",
      "  Gen: Minnesota Golden Gophers\n",
      "\n",
      "\n",
      "--- Calculating ROUGE Scores ---\n",
      "Calculating ROUGE for Greedy Search...\n",
      "Calculating ROUGE for Beam Search (beams=4)...\n",
      "\n",
      "--- Evaluation Results for Fine-tuned T5-Small ---\n",
      "Generation Time (Greedy): 6.90s\n",
      "ROUGE Scores (Greedy):\n",
      "  ROUGE-1 F1: 90.38\n",
      "  ROUGE-2 F1: 69.96\n",
      "  ROUGE-L F1: 90.36\n",
      "------------------------------\n",
      "Generation Time (Beam Search, beams=4): 9.84s\n",
      "ROUGE Scores (Beam Search, beams=4):\n",
      "  ROUGE-1 F1: 89.70\n",
      "  ROUGE-2 F1: 69.31\n",
      "  ROUGE-L F1: 89.64\n",
      "\n",
      "--- End of C1 Evaluation ---\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from tqdm.notebook import tqdm # Or from tqdm import tqdm\n",
    "import time\n",
    "import evaluate # Use Hugging Face's evaluate library\n",
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "# --- 1. Load Test Data ---\n",
    "test_data_path = \"/kaggle/input/assignment2nlp/test.csv\" # CHANGE TO YOUR PATH\n",
    "print(f\"Loading test data from: {test_data_path}\")\n",
    "try:\n",
    "    # Load using datasets library to be consistent if train/val were loaded that way\n",
    "    # Or use pandas: test_df = pd.read_csv(test_data_path)\n",
    "    raw_test_dataset = load_dataset(\"csv\", data_files={\"test\": test_data_path})[\"test\"]\n",
    "\n",
    "    # Extract texts and reference titles\n",
    "    # Handle potential None values\n",
    "    test_texts = [str(t) if t is not None else \"\" for t in raw_test_dataset['text']]\n",
    "    reference_titles = [str(t) if t is not None else \"\" for t in raw_test_dataset['title']]\n",
    "    print(f\"Loaded {len(test_texts)} test examples.\")\n",
    "    # Optional: Use a smaller subset for faster debugging\n",
    "    # subset_size = 10\n",
    "    # test_texts = test_texts[:subset_size]\n",
    "    # reference_titles = reference_titles[:subset_size]\n",
    "    # print(f\"Using subset of {len(test_texts)} examples for testing.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading or processing test data: {e}\")\n",
    "    raise\n",
    "\n",
    "# --- 2. Ensure Model and Tokenizer are Ready ---\n",
    "# Assuming 'trainer' and 'tokenizer' are still available from the training step\n",
    "# If not, load them:\n",
    "# model_path = \"./results_c1_t5_small\" # Path where trainer saved the model\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "# model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model.to(device)\n",
    "\n",
    "# Get the fine-tuned model from the trainer\n",
    "model = trainer.model\n",
    "model.eval() # Set model to evaluation mode\n",
    "\n",
    "# Define the prefix used during training (if any)\n",
    "prefix = \"summarize: \" # Make sure this matches the prefix used in tokenize_function\n",
    "\n",
    "# --- 3. Generation Function (can be adapted from C2) ---\n",
    "def generate_predictions(model, tokenizer, texts, prefix, device, num_beams=1, max_length=64):\n",
    "    \"\"\"Generates titles for a list of texts.\"\"\"\n",
    "    generated_titles = []\n",
    "    model.eval() # Ensure model is in eval mode\n",
    "    with torch.no_grad():\n",
    "        for text in tqdm(texts, desc=f\"Generating (beams={num_beams})\"):\n",
    "            # Prepare input\n",
    "            input_text = prefix + text\n",
    "            inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True, padding=True)\n",
    "            inputs = inputs.to(device)\n",
    "\n",
    "            # Generate\n",
    "            output_sequences = model.generate(\n",
    "                input_ids=inputs['input_ids'],\n",
    "                attention_mask=inputs['attention_mask'],\n",
    "                max_new_tokens=max_length,\n",
    "                num_beams=num_beams,\n",
    "                early_stopping=True if num_beams > 1 else False # Early stopping mainly for beam search\n",
    "            )\n",
    "\n",
    "            # Decode\n",
    "            prediction = tokenizer.decode(output_sequences[0], skip_special_tokens=True)\n",
    "            generated_titles.append(prediction)\n",
    "    return generated_titles\n",
    "\n",
    "# --- 4. Generate Predictions - Greedy Search ---\n",
    "print(\"\\n--- Generating predictions (Greedy Search) ---\")\n",
    "start_time_greedy = time.time()\n",
    "greedy_predictions = generate_predictions(model, tokenizer, test_texts, prefix, device, num_beams=1)\n",
    "end_time_greedy = time.time()\n",
    "time_greedy = end_time_greedy - start_time_greedy\n",
    "print(f\"Greedy generation finished in {time_greedy:.2f} seconds.\")\n",
    "print(\"Example Greedy Predictions:\")\n",
    "for i in range(min(3, len(greedy_predictions))):\n",
    "    print(f\"  Ref: {reference_titles[i]}\")\n",
    "    print(f\"  Gen: {greedy_predictions[i]}\\n\")\n",
    "\n",
    "\n",
    "# --- 5. Generate Predictions - Beam Search ---\n",
    "BEAM_SIZE = 4 # You can choose a different beam size\n",
    "print(f\"\\n--- Generating predictions (Beam Search, Num Beams = {BEAM_SIZE}) ---\")\n",
    "start_time_beam = time.time()\n",
    "beam_predictions = generate_predictions(model, tokenizer, test_texts, prefix, device, num_beams=BEAM_SIZE)\n",
    "end_time_beam = time.time()\n",
    "time_beam = end_time_beam - start_time_beam\n",
    "print(f\"Beam search generation finished in {time_beam:.2f} seconds.\")\n",
    "print(\"Example Beam Search Predictions:\")\n",
    "for i in range(min(3, len(beam_predictions))):\n",
    "    print(f\"  Ref: {reference_titles[i]}\")\n",
    "    print(f\"  Gen: {beam_predictions[i]}\\n\")\n",
    "\n",
    "\n",
    "# --- 6. Calculate ROUGE Scores ---\n",
    "print(\"\\n--- Calculating ROUGE Scores ---\")\n",
    "\n",
    "# Ensure NLTK punkt is downloaded\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except (OSError, LookupError):\n",
    "    print(\"Downloading nltk punkt tokenizer...\")\n",
    "    nltk.download('punkt', quiet=True)\n",
    "\n",
    "# Load the ROUGE Metric\n",
    "try:\n",
    "    rouge_metric = evaluate.load('rouge')\n",
    "except Exception as e:\n",
    "    print(f\"Error loading ROUGE metric: {e}\")\n",
    "    raise\n",
    "\n",
    "# Re-use the calculation function (or define it here if not run before)\n",
    "def calculate_rouge_scores(predictions, references):\n",
    "    \"\"\"Calculates ROUGE scores using the evaluate library.\"\"\"\n",
    "    if not predictions or not references or len(predictions) != len(references):\n",
    "        print(\"Warning: Invalid input for ROUGE calculation.\")\n",
    "        return None\n",
    "    result = rouge_metric.compute(predictions=predictions, references=references, use_stemmer=True)\n",
    "    scores = {\n",
    "        'ROUGE-1 F1': result.get('rouge1', 0.0) * 100,\n",
    "        'ROUGE-2 F1': result.get('rouge2', 0.0) * 100,\n",
    "        'ROUGE-L F1': result.get('rougeL', 0.0) * 100,\n",
    "    }\n",
    "    return scores\n",
    "\n",
    "# Calculate for Greedy\n",
    "print(\"Calculating ROUGE for Greedy Search...\")\n",
    "rouge_scores_greedy = calculate_rouge_scores(greedy_predictions, reference_titles)\n",
    "\n",
    "# Calculate for Beam Search\n",
    "print(f\"Calculating ROUGE for Beam Search (beams={BEAM_SIZE})...\")\n",
    "rouge_scores_beam = calculate_rouge_scores(beam_predictions, reference_titles)\n",
    "\n",
    "# --- 7. Print Results ---\n",
    "print(\"\\n--- Evaluation Results for Fine-tuned T5-Small ---\")\n",
    "print(f\"Generation Time (Greedy): {time_greedy:.2f}s\")\n",
    "if rouge_scores_greedy:\n",
    "    print(\"ROUGE Scores (Greedy):\")\n",
    "    print(f\"  ROUGE-1 F1: {rouge_scores_greedy['ROUGE-1 F1']:.2f}\")\n",
    "    print(f\"  ROUGE-2 F1: {rouge_scores_greedy['ROUGE-2 F1']:.2f}\")\n",
    "    print(f\"  ROUGE-L F1: {rouge_scores_greedy['ROUGE-L F1']:.2f}\")\n",
    "else:\n",
    "    print(\"ROUGE Scores (Greedy): Not calculated.\")\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(f\"Generation Time (Beam Search, beams={BEAM_SIZE}): {time_beam:.2f}s\")\n",
    "if rouge_scores_beam:\n",
    "    print(f\"ROUGE Scores (Beam Search, beams={BEAM_SIZE}):\")\n",
    "    print(f\"  ROUGE-1 F1: {rouge_scores_beam['ROUGE-1 F1']:.2f}\")\n",
    "    print(f\"  ROUGE-2 F1: {rouge_scores_beam['ROUGE-2 F1']:.2f}\")\n",
    "    print(f\"  ROUGE-L F1: {rouge_scores_beam['ROUGE-L F1']:.2f}\")\n",
    "else:\n",
    "    print(f\"ROUGE Scores (Beam Search, beams={BEAM_SIZE}): Not calculated.\")\n",
    "\n",
    "print(\"\\n--- End of C1 Evaluation ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T15:28:44.794737Z",
     "iopub.status.busy": "2025-04-13T15:28:44.793945Z",
     "iopub.status.idle": "2025-04-13T15:29:04.115519Z",
     "shell.execute_reply": "2025-04-13T15:29:04.114664Z",
     "shell.execute_reply.started": "2025-04-13T15:28:44.794710Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading google/flan-t5-base...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92e42e8c918b422b9285911e091161e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  56%|#####6    | 556M/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "913720479d6e4be89e357dccdf6ec452",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "google/flan-t5-base loaded successfully.\n",
      "Loading google/flan-t5-large...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51b00a73820043269e708a47d8f29ae2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d8d813312524f56b7bb1a09e3391e6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b33329f3d9d740ae8349c42b20de662f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d78dec6e8281442fb9da7ec91fd43dcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec08725db294450ca1881f970ec1ef4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/662 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca9ef0d3e2b14ac282b7cb601dc81312",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.13G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29033455bbb34e5baeea6e52b5c69ce3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "google/flan-t5-large loaded successfully.\n",
      "Flan-T5 Base is ready.\n",
      "Flan-T5 Large is ready.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "import torch # Import torch to check for GPU\n",
    "\n",
    "# --- Define Model Names ---\n",
    "model_name_base = \"google/flan-t5-base\"\n",
    "model_name_large = \"google/flan-t5-large\"\n",
    "\n",
    "# --- Check for GPU availability ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- Load Flan-T5 Base Model and Tokenizer ---\n",
    "print(f\"Loading {model_name_base}...\")\n",
    "try:\n",
    "    tokenizer_base = AutoTokenizer.from_pretrained(model_name_base)\n",
    "    model_base = AutoModelForSeq2SeqLM.from_pretrained(model_name_base)\n",
    "    model_base.to(device) # Move model to GPU if available\n",
    "    print(f\"{model_name_base} loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading {model_name_base}: {e}\")\n",
    "    # Depending on your setup, you might want to stop or continue without this model\n",
    "    model_base = None\n",
    "    tokenizer_base = None\n",
    "\n",
    "# --- Load Flan-T5 Large Model and Tokenizer ---\n",
    "# Note: flan-t5-large is significantly bigger and requires more memory/GPU RAM.\n",
    "# If you have resource constraints, you might skip this or use it cautiously.\n",
    "print(f\"Loading {model_name_large}...\")\n",
    "try:\n",
    "    tokenizer_large = AutoTokenizer.from_pretrained(model_name_large)\n",
    "    model_large = AutoModelForSeq2SeqLM.from_pretrained(model_name_large)\n",
    "    model_large.to(device) # Move model to GPU if available\n",
    "    print(f\"{model_name_large} loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading {model_name_large}: {e}\")\n",
    "    # Depending on your setup, you might want to stop or continue without this model\n",
    "    model_large = None\n",
    "    tokenizer_large = None\n",
    "\n",
    "# --- Verify loaded models ---\n",
    "if model_base and tokenizer_base:\n",
    "    print(\"Flan-T5 Base is ready.\")\n",
    "if model_large and tokenizer_large:\n",
    "    print(\"Flan-T5 Large is ready.\")\n",
    "elif model_name_large: # Check if we attempted to load it\n",
    "     print(\"Flan-T5 Large could not be loaded (check memory/GPU RAM).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T15:29:28.428712Z",
     "iopub.status.busy": "2025-04-13T15:29:28.428101Z",
     "iopub.status.idle": "2025-04-13T15:29:28.466018Z",
     "shell.execute_reply": "2025-04-13T15:29:28.465272Z",
     "shell.execute_reply.started": "2025-04-13T15:29:28.428687Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded test data with 100 examples.\n",
      "Defined prompts:\n",
      "- Prompt 1 (Generate): 'Generate a concise title for the following Wikiped...'\n",
      "- Prompt 2 (Question): 'What is a suitable title for this text? Text: {art...'\n",
      "- Prompt 3 (Summarize): 'Summarize the main topic of this document into a s...'\n",
      "Ready to generate titles using models: ['flan-t5-base', 'flan-t5-large']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_75/3235236602.py:18: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  test_df['text'].fillna('', inplace=True)\n",
      "/tmp/ipykernel_75/3235236602.py:19: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  test_df['title'].fillna('', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# --- 1. Load the RAW Test Data ---\n",
    "# Make sure the path to your test.csv is correct\n",
    "test_data_path = \"/kaggle/input/assignment2nlp/test.csv\" # Example path\n",
    "\n",
    "if not os.path.exists(test_data_path):\n",
    "    raise FileNotFoundError(f\"Test file not found at: {test_data_path}\")\n",
    "\n",
    "# Load using pandas to easily access text and title columns\n",
    "try:\n",
    "    test_df = pd.read_csv(test_data_path)\n",
    "    # Ensure 'text' and 'title' columns exist\n",
    "    if 'text' not in test_df.columns or 'title' not in test_df.columns:\n",
    "        raise ValueError(\"Test CSV must contain 'text' and 'title' columns.\")\n",
    "    # Handle potential missing values (replace NaN with empty string)\n",
    "    test_df['text'].fillna('', inplace=True)\n",
    "    test_df['title'].fillna('', inplace=True)\n",
    "    print(f\"Loaded test data with {len(test_df)} examples.\")\n",
    "    # Keep only a small subset for faster testing/debugging if needed\n",
    "    # test_df = test_df.head(5)\n",
    "    # print(f\"Using subset of {len(test_df)} examples for testing.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading or processing test data: {e}\")\n",
    "    raise\n",
    "\n",
    "# Extract texts and reference titles for later evaluation\n",
    "test_texts = test_df['text'].tolist()\n",
    "reference_titles = test_df['title'].tolist() # Ground truth titles\n",
    "\n",
    "# --- 2. Define Prompt Variations ---\n",
    "# You need at least two variations. Here are a few examples:\n",
    "\n",
    "prompt1_template = \"Generate a concise title for the following Wikipedia article: {article_text}\"\n",
    "\n",
    "prompt2_template = \"What is a suitable title for this text? Text: {article_text} Title:\"\n",
    "\n",
    "prompt3_template = \"Summarize the main topic of this document into a short title: {article_text}\"\n",
    "# Add more prompts if you like\n",
    "\n",
    "# Store prompts for easy iteration\n",
    "prompts = {\n",
    "    \"Prompt 1 (Generate)\": prompt1_template,\n",
    "    \"Prompt 2 (Question)\": prompt2_template,\n",
    "    \"Prompt 3 (Summarize)\": prompt3_template,\n",
    "    # Add more keys if you added more prompts\n",
    "}\n",
    "\n",
    "print(\"Defined prompts:\")\n",
    "for name, template in prompts.items():\n",
    "    print(f\"- {name}: '{template[:50]}...'\") # Print start of each template\n",
    "\n",
    "# --- Store models and tokenizers for iteration ---\n",
    "# Assumes model_base, tokenizer_base, model_large, tokenizer_large are loaded from previous step\n",
    "# And device is also defined ('cuda' or 'cpu')\n",
    "available_models = {}\n",
    "if 'model_base' in locals() and model_base is not None:\n",
    "    available_models['flan-t5-base'] = {'model': model_base, 'tokenizer': tokenizer_base}\n",
    "if 'model_large' in locals() and model_large is not None:\n",
    "    available_models['flan-t5-large'] = {'model': model_large, 'tokenizer': tokenizer_large}\n",
    "\n",
    "if not available_models:\n",
    "    raise RuntimeError(\"No Flan-T5 models were successfully loaded in the previous step.\")\n",
    "\n",
    "print(f\"Ready to generate titles using models: {list(available_models.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T15:29:34.894898Z",
     "iopub.status.busy": "2025-04-13T15:29:34.894622Z",
     "iopub.status.idle": "2025-04-13T15:29:34.930474Z",
     "shell.execute_reply": "2025-04-13T15:29:34.929591Z",
     "shell.execute_reply.started": "2025-04-13T15:29:34.894876Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded test data with 100 examples.\n",
      "Defined prompts:\n",
      "- Prompt 1 (Generate): 'Generate a concise title for the following Wikiped...'\n",
      "- Prompt 2 (Question): 'What is a suitable title for this text? Text: {art...'\n",
      "- Prompt 3 (Summarize): 'Summarize the main topic of this document into a s...'\n",
      "Ready to generate titles using models: ['flan-t5-base', 'flan-t5-large']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_75/3235236602.py:18: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  test_df['text'].fillna('', inplace=True)\n",
      "/tmp/ipykernel_75/3235236602.py:19: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  test_df['title'].fillna('', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# --- 1. Load the RAW Test Data ---\n",
    "# Make sure the path to your test.csv is correct\n",
    "test_data_path = \"/kaggle/input/assignment2nlp/test.csv\" # Example path\n",
    "\n",
    "if not os.path.exists(test_data_path):\n",
    "    raise FileNotFoundError(f\"Test file not found at: {test_data_path}\")\n",
    "\n",
    "# Load using pandas to easily access text and title columns\n",
    "try:\n",
    "    test_df = pd.read_csv(test_data_path)\n",
    "    # Ensure 'text' and 'title' columns exist\n",
    "    if 'text' not in test_df.columns or 'title' not in test_df.columns:\n",
    "        raise ValueError(\"Test CSV must contain 'text' and 'title' columns.\")\n",
    "    # Handle potential missing values (replace NaN with empty string)\n",
    "    test_df['text'].fillna('', inplace=True)\n",
    "    test_df['title'].fillna('', inplace=True)\n",
    "    print(f\"Loaded test data with {len(test_df)} examples.\")\n",
    "    # Keep only a small subset for faster testing/debugging if needed\n",
    "    # test_df = test_df.head(5)\n",
    "    # print(f\"Using subset of {len(test_df)} examples for testing.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading or processing test data: {e}\")\n",
    "    raise\n",
    "\n",
    "# Extract texts and reference titles for later evaluation\n",
    "test_texts = test_df['text'].tolist()\n",
    "reference_titles = test_df['title'].tolist() # Ground truth titles\n",
    "\n",
    "# --- 2. Define Prompt Variations ---\n",
    "# You need at least two variations. Here are a few examples:\n",
    "\n",
    "prompt1_template = \"Generate a concise title for the following Wikipedia article: {article_text}\"\n",
    "\n",
    "prompt2_template = \"What is a suitable title for this text? Text: {article_text} Title:\"\n",
    "\n",
    "prompt3_template = \"Summarize the main topic of this document into a short title: {article_text}\"\n",
    "# Add more prompts if you like\n",
    "\n",
    "# Store prompts for easy iteration\n",
    "prompts = {\n",
    "    \"Prompt 1 (Generate)\": prompt1_template,\n",
    "    \"Prompt 2 (Question)\": prompt2_template,\n",
    "    \"Prompt 3 (Summarize)\": prompt3_template,\n",
    "    # Add more keys if you added more prompts\n",
    "}\n",
    "\n",
    "print(\"Defined prompts:\")\n",
    "for name, template in prompts.items():\n",
    "    print(f\"- {name}: '{template[:50]}...'\") # Print start of each template\n",
    "\n",
    "# --- Store models and tokenizers for iteration ---\n",
    "# Assumes model_base, tokenizer_base, model_large, tokenizer_large are loaded from previous step\n",
    "# And device is also defined ('cuda' or 'cpu')\n",
    "available_models = {}\n",
    "if 'model_base' in locals() and model_base is not None:\n",
    "    available_models['flan-t5-base'] = {'model': model_base, 'tokenizer': tokenizer_base}\n",
    "if 'model_large' in locals() and model_large is not None:\n",
    "    available_models['flan-t5-large'] = {'model': model_large, 'tokenizer': tokenizer_large}\n",
    "\n",
    "if not available_models:\n",
    "    raise RuntimeError(\"No Flan-T5 models were successfully loaded in the previous step.\")\n",
    "\n",
    "print(f\"Ready to generate titles using models: {list(available_models.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T15:29:45.537898Z",
     "iopub.status.busy": "2025-04-13T15:29:45.537612Z",
     "iopub.status.idle": "2025-04-13T15:32:23.943468Z",
     "shell.execute_reply": "2025-04-13T15:32:23.942220Z",
     "shell.execute_reply.started": "2025-04-13T15:29:45.537878Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Generating titles with flan-t5-base ---\n",
      "Using prompt: 'Prompt 1 (Generate)'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "498f1026d512477f88bef841e62c4bd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "flan-t5-base - Prompt 1 (Generate):   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:679: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished 'Prompt 1 (Generate)' for flan-t5-base in 17.15 seconds.\n",
      "Example generated titles:\n",
      "  Article 1: Weyburn, Saskatchewan\n",
      "  Article 2: Catholic High School\n",
      "  Article 3: Minnesota Golden Gophers\n",
      "Using prompt: 'Prompt 2 (Question)'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63929eccbf36492bb644fffcc56457c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "flan-t5-base - Prompt 2 (Question):   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished 'Prompt 2 (Question)' for flan-t5-base in 15.87 seconds.\n",
      "Example generated titles:\n",
      "  Article 1: Weyburn, Saskatchewan\n",
      "  Article 2: Catholic High School\n",
      "  Article 3: Sports\n",
      "Using prompt: 'Prompt 3 (Summarize)'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3707689842a4080ba50410a37fbb6db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "flan-t5-base - Prompt 3 (Summarize):   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished 'Prompt 3 (Summarize)' for flan-t5-base in 19.25 seconds.\n",
      "Example generated titles:\n",
      "  Article 1: Weyburn, Saskatchewan\n",
      "  Article 2: Catholic High School\n",
      "  Article 3: University of Minnesota\n",
      "\n",
      "--- Generating titles with flan-t5-large ---\n",
      "Using prompt: 'Prompt 1 (Generate)'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cba0faa7793430f9582fe37cf62a4aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "flan-t5-large - Prompt 1 (Generate):   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished 'Prompt 1 (Generate)' for flan-t5-large in 35.24 seconds.\n",
      "Example generated titles:\n",
      "  Article 1: Weyburn, Saskatchewan\n",
      "  Article 2: Catholic High School\n",
      "  Article 3: University of Minnesota\n",
      "Using prompt: 'Prompt 2 (Question)'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b14f31e0b23b4eb88ef4da88e6da13e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "flan-t5-large - Prompt 2 (Question):   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished 'Prompt 2 (Question)' for flan-t5-large in 35.04 seconds.\n",
      "Example generated titles:\n",
      "  Article 1: Weyburn, Saskatchewan\n",
      "  Article 2: Catholic High School\n",
      "  Article 3: University of Minnesota\n",
      "Using prompt: 'Prompt 3 (Summarize)'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5ae9a67b4454eee9aff4859da37e755",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "flan-t5-large - Prompt 3 (Summarize):   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished 'Prompt 3 (Summarize)' for flan-t5-large in 35.85 seconds.\n",
      "Example generated titles:\n",
      "  Article 1: Weyburn, Saskatchewan\n",
      "  Article 2: Catholic High School\n",
      "  Article 3: University of Minnesota\n",
      "\n",
      "--- All generations complete ---\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm.notebook import tqdm # Use tqdm.notebook for Kaggle/Jupyter, or just tqdm otherwise\n",
    "import time\n",
    "\n",
    "# --- Generation Function ---\n",
    "def generate_title_with_prompt(model, tokenizer, prompt_template, article_text, device, max_length=64, num_beams=1):\n",
    "    \"\"\"\n",
    "    Generates a title for a given article using a specific prompt and model.\n",
    "\n",
    "    Args:\n",
    "        model: The loaded Seq2Seq LM model.\n",
    "        tokenizer: The corresponding tokenizer.\n",
    "        prompt_template: The string template for the prompt (e.g., \"Generate title: {article_text}\").\n",
    "        article_text: The raw text of the article.\n",
    "        device: The device ('cuda' or 'cpu') to run generation on.\n",
    "        max_length: The maximum length of the generated title tokens.\n",
    "        num_beams: Number of beams for beam search (1 = greedy).\n",
    "\n",
    "    Returns:\n",
    "        The generated title string.\n",
    "    \"\"\"\n",
    "    # Format the prompt\n",
    "    prompted_text = prompt_template.format(article_text=article_text)\n",
    "\n",
    "    # Tokenize the prompted text\n",
    "    # Flan-T5 doesn't strictly require a prefix like T5, but the prompt serves a similar role.\n",
    "    # We need to handle potential length issues here before generation.\n",
    "    # Let's tokenize first to see the length, though generation handles truncation internally.\n",
    "    inputs = tokenizer(prompted_text, return_tensors=\"pt\", truncation=True, max_length=1024) # Flan-T5 often has longer context\n",
    "    inputs = inputs.to(device) # Move tokenized inputs to the correct device\n",
    "\n",
    "    # Generate output (title)\n",
    "    # Use torch.no_grad() for inference to save memory and compute\n",
    "    with torch.no_grad():\n",
    "        output_sequences = model.generate(\n",
    "            input_ids=inputs['input_ids'],\n",
    "            attention_mask=inputs['attention_mask'], # Pass attention mask\n",
    "            max_new_tokens=max_length,              # More modern way than max_length for generated part\n",
    "            num_beams=num_beams,                    # Control greedy vs beam search\n",
    "            early_stopping=True                     # Stop beam search early if possible\n",
    "        )\n",
    "\n",
    "    # Decode the generated token ids to text\n",
    "    generated_title = tokenizer.decode(output_sequences[0], skip_special_tokens=True)\n",
    "\n",
    "    return generated_title\n",
    "\n",
    "# --- Run Generation for All Models and Prompts ---\n",
    "\n",
    "# Dictionary to store results: results[model_name][prompt_name] = [list_of_titles]\n",
    "results = {}\n",
    "generation_times = {} # To store time taken\n",
    "\n",
    "# Iterate through each available model\n",
    "for model_name, model_info in available_models.items():\n",
    "    print(f\"\\n--- Generating titles with {model_name} ---\")\n",
    "    model = model_info['model']\n",
    "    tokenizer = model_info['tokenizer']\n",
    "    results[model_name] = {}\n",
    "    generation_times[model_name] = {}\n",
    "\n",
    "    # Iterate through each prompt\n",
    "    for prompt_name, prompt_template in prompts.items():\n",
    "        print(f\"Using prompt: '{prompt_name}'\")\n",
    "        start_time = time.time()\n",
    "        generated_titles_list = []\n",
    "        # Use tqdm for progress bar over test texts\n",
    "        for article_text in tqdm(test_texts, desc=f\"{model_name} - {prompt_name}\"):\n",
    "            # Generate title for the current article text\n",
    "            # Using default greedy search (num_beams=1) for now\n",
    "            # You might want to run beam search separately if needed\n",
    "            generated_title = generate_title_with_prompt(\n",
    "                model,\n",
    "                tokenizer,\n",
    "                prompt_template,\n",
    "                article_text,\n",
    "                device,\n",
    "                max_length=64, # Max length for the generated title\n",
    "                num_beams=1    # Greedy decoding\n",
    "            )\n",
    "            generated_titles_list.append(generated_title)\n",
    "\n",
    "        end_time = time.time()\n",
    "        total_time = end_time - start_time\n",
    "        results[model_name][prompt_name] = generated_titles_list\n",
    "        generation_times[model_name][prompt_name] = total_time\n",
    "        print(f\"Finished '{prompt_name}' for {model_name} in {total_time:.2f} seconds.\")\n",
    "        # Optional: Print a few examples\n",
    "        print(\"Example generated titles:\")\n",
    "        for i in range(min(3, len(test_texts))):\n",
    "             print(f\"  Article {i+1}: {generated_titles_list[i]}\")\n",
    "\n",
    "print(\"\\n--- All generations complete ---\")\n",
    "# Results dictionary now contains all generated titles\n",
    "# Example access: results['flan-t5-base']['Prompt 1 (Generate)'][0] -> first title generated by base model with prompt 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-13T15:32:33.878206Z",
     "iopub.status.busy": "2025-04-13T15:32:33.877878Z",
     "iopub.status.idle": "2025-04-13T15:32:35.536802Z",
     "shell.execute_reply": "2025-04-13T15:32:35.536142Z",
     "shell.execute_reply.started": "2025-04-13T15:32:33.878176Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE metric loaded successfully.\n",
      "\n",
      "--- Calculating ROUGE Scores for Flan-T5 Generations ---\n",
      "\n",
      "Evaluating Model: flan-t5-base\n",
      "  Prompt: Prompt 1 (Generate)\n",
      "    ROUGE-1 F1: 85.57\n",
      "    ROUGE-2 F1: 66.32\n",
      "    ROUGE-L F1: 85.26\n",
      "  Prompt: Prompt 2 (Question)\n",
      "    ROUGE-1 F1: 69.28\n",
      "    ROUGE-2 F1: 54.24\n",
      "    ROUGE-L F1: 69.37\n",
      "  Prompt: Prompt 3 (Summarize)\n",
      "    ROUGE-1 F1: 75.09\n",
      "    ROUGE-2 F1: 54.86\n",
      "    ROUGE-L F1: 73.80\n",
      "\n",
      "Evaluating Model: flan-t5-large\n",
      "  Prompt: Prompt 1 (Generate)\n",
      "    ROUGE-1 F1: 88.23\n",
      "    ROUGE-2 F1: 64.78\n",
      "    ROUGE-L F1: 88.15\n",
      "  Prompt: Prompt 2 (Question)\n",
      "    ROUGE-1 F1: 87.96\n",
      "    ROUGE-2 F1: 66.00\n",
      "    ROUGE-L F1: 87.86\n",
      "  Prompt: Prompt 3 (Summarize)\n",
      "    ROUGE-1 F1: 88.88\n",
      "    ROUGE-2 F1: 66.42\n",
      "    ROUGE-L F1: 88.82\n",
      "\n",
      "--- ROUGE Score Calculation Complete ---\n",
      "\n",
      "Summary of ROUGE F1 Scores (%):\n",
      "--------------------------------------------------\n",
      "Model: flan-t5-base\n",
      "  Prompt: Prompt 1 (Generate)       | ROUGE-1: 85.57 | ROUGE-2: 66.32 | ROUGE-L: 85.26\n",
      "  Prompt: Prompt 2 (Question)       | ROUGE-1: 69.28 | ROUGE-2: 54.24 | ROUGE-L: 69.37\n",
      "  Prompt: Prompt 3 (Summarize)      | ROUGE-1: 75.09 | ROUGE-2: 54.86 | ROUGE-L: 73.80\n",
      "--------------------------------------------------\n",
      "Model: flan-t5-large\n",
      "  Prompt: Prompt 1 (Generate)       | ROUGE-1: 88.23 | ROUGE-2: 64.78 | ROUGE-L: 88.15\n",
      "  Prompt: Prompt 2 (Question)       | ROUGE-1: 87.96 | ROUGE-2: 66.00 | ROUGE-L: 87.86\n",
      "  Prompt: Prompt 3 (Summarize)      | ROUGE-1: 88.88 | ROUGE-2: 66.42 | ROUGE-L: 88.82\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import evaluate # Use Hugging Face's evaluate library\n",
    "import numpy as np\n",
    "import nltk # Ensure nltk is imported, needed by rouge_score\n",
    "\n",
    "# --- Ensure NLTK punkt is downloaded (needed for default ROUGE tokenizer) ---\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except (OSError, LookupError):\n",
    "    print(\"Downloading nltk punkt tokenizer...\")\n",
    "    nltk.download('punkt', quiet=True)\n",
    "\n",
    "# --- Load the ROUGE Metric ---\n",
    "try:\n",
    "    rouge_metric = evaluate.load('rouge')\n",
    "    print(\"ROUGE metric loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading ROUGE metric: {e}\")\n",
    "    print(\"Please ensure the 'evaluate' and 'rouge_score' libraries are installed (`pip install evaluate rouge_score`)\")\n",
    "    # Stop execution if metric can't be loaded\n",
    "    raise\n",
    "\n",
    "# --- Function to Compute ROUGE Scores ---\n",
    "def calculate_rouge_scores(predictions, references):\n",
    "    \"\"\"Calculates ROUGE scores using the evaluate library.\"\"\"\n",
    "    if not predictions or not references or len(predictions) != len(references):\n",
    "        print(\"Warning: Invalid input for ROUGE calculation (empty lists or length mismatch).\")\n",
    "        return None\n",
    "\n",
    "    # The library expects lists of strings\n",
    "    result = rouge_metric.compute(predictions=predictions, references=references, use_stemmer=True)\n",
    "\n",
    "    # Extract F1 scores (you can also extract precision/recall if needed)\n",
    "    # The keys might be rouge1, rouge2, rougeL, rougeLsum depending on the version and options\n",
    "    # We'll check for common keys and report F1 score\n",
    "    scores = {\n",
    "        'ROUGE-1 F1': result.get('rouge1', 0.0) * 100, # Multiply by 100 for percentage\n",
    "        'ROUGE-2 F1': result.get('rouge2', 0.0) * 100,\n",
    "        'ROUGE-L F1': result.get('rougeL', 0.0) * 100,\n",
    "    }\n",
    "    return scores\n",
    "\n",
    "# --- Evaluate All Generated Titles ---\n",
    "\n",
    "# Store results in a dictionary for easy reporting\n",
    "rouge_results_c2 = {}\n",
    "\n",
    "print(\"\\n--- Calculating ROUGE Scores for Flan-T5 Generations ---\")\n",
    "\n",
    "# Ensure reference_titles list is available from the previous step\n",
    "if 'reference_titles' not in locals() or not reference_titles:\n",
    "     raise NameError(\"The 'reference_titles' list is not defined. Please ensure the test data loading was successful.\")\n",
    "if 'results' not in locals() or not results:\n",
    "     raise NameError(\"The 'results' dictionary with generated titles is not defined. Please ensure the generation step ran successfully.\")\n",
    "\n",
    "\n",
    "# Iterate through models and prompts where results were generated\n",
    "for model_name, prompt_dict in results.items():\n",
    "    rouge_results_c2[model_name] = {}\n",
    "    print(f\"\\nEvaluating Model: {model_name}\")\n",
    "    for prompt_name, generated_titles in prompt_dict.items():\n",
    "        print(f\"  Prompt: {prompt_name}\")\n",
    "\n",
    "        # Ensure we have the same number of predictions and references\n",
    "        if len(generated_titles) != len(reference_titles):\n",
    "            print(f\"    Warning: Mismatch in number of generated ({len(generated_titles)}) and reference ({len(reference_titles)}) titles. Skipping.\")\n",
    "            rouge_results_c2[model_name][prompt_name] = None\n",
    "            continue\n",
    "\n",
    "        # Calculate scores\n",
    "        scores = calculate_rouge_scores(generated_titles, reference_titles)\n",
    "\n",
    "        if scores:\n",
    "            rouge_results_c2[model_name][prompt_name] = scores\n",
    "            # Print scores formatted to 2 decimal places\n",
    "            print(f\"    ROUGE-1 F1: {scores['ROUGE-1 F1']:.2f}\")\n",
    "            print(f\"    ROUGE-2 F1: {scores['ROUGE-2 F1']:.2f}\")\n",
    "            print(f\"    ROUGE-L F1: {scores['ROUGE-L F1']:.2f}\")\n",
    "        else:\n",
    "            print(\"    Failed to calculate ROUGE scores.\")\n",
    "            rouge_results_c2[model_name][prompt_name] = None\n",
    "\n",
    "\n",
    "print(\"\\n--- ROUGE Score Calculation Complete ---\")\n",
    "\n",
    "# Now rouge_results_c2 dictionary holds the scores, e.g.:\n",
    "# rouge_results_c2['flan-t5-base']['Prompt 1 (Generate)']['ROUGE-1 F1']\n",
    "\n",
    "# You would typically format these results into a table for your report (using pandas or just printing)\n",
    "print(\"\\nSummary of ROUGE F1 Scores (%):\")\n",
    "print(\"-\" * 50)\n",
    "for model_name, prompt_scores in rouge_results_c2.items():\n",
    "    print(f\"Model: {model_name}\")\n",
    "    for prompt_name, scores in prompt_scores.items():\n",
    "         if scores:\n",
    "             print(f\"  Prompt: {prompt_name:<25} | ROUGE-1: {scores['ROUGE-1 F1']:.2f} | ROUGE-2: {scores['ROUGE-2 F1']:.2f} | ROUGE-L: {scores['ROUGE-L F1']:.2f}\")\n",
    "         else:\n",
    "             print(f\"  Prompt: {prompt_name:<25} | Scores: Not calculated\")\n",
    "    print(\"-\" * 50)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 870709,
     "sourceId": 1483651,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7129532,
     "sourceId": 11385756,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

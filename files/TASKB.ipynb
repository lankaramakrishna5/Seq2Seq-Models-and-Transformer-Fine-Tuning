{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-13T12:06:04.546198Z",
     "iopub.status.busy": "2025-04-13T12:06:04.545536Z",
     "iopub.status.idle": "2025-04-13T12:06:07.109287Z",
     "shell.execute_reply": "2025-04-13T12:06:07.108717Z",
     "shell.execute_reply.started": "2025-04-13T12:06:04.546174Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import os\n",
    "import pickle # Or json, if you saved vocab that way\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize # For Hierarchical Encoder\n",
    "from rouge_score import rouge_scorer # For evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-13T12:06:07.110944Z",
     "iopub.status.busy": "2025-04-13T12:06:07.110581Z",
     "iopub.status.idle": "2025-04-13T12:06:07.317574Z",
     "shell.execute_reply": "2025-04-13T12:06:07.316897Z",
     "shell.execute_reply.started": "2025-04-13T12:06:07.110925Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "# File Paths (Update these)\n",
    "PROCESSED_DATA_DIR = '/kaggle/input/processedassignment2datanlp' # Directory containing outputs from taskA\n",
    "TRAIN_PROC_FILE = os.path.join(PROCESSED_DATA_DIR, 'train_processed.csv')\n",
    "VAL_PROC_FILE = os.path.join(PROCESSED_DATA_DIR, 'validation_processed.csv')\n",
    "TEST_PROC_FILE = os.path.join(PROCESSED_DATA_DIR, 'test_processed.csv')\n",
    "VOCAB_FILE = os.path.join(PROCESSED_DATA_DIR, 'vocabulary.pkl') # Or .json if saved that way\n",
    "GLOVE_FILE = '/kaggle/input/glove-embeddings/glove.6B.300d.txt' # <--- CHANGE THIS (e.g., glove.6B.300d.txt)\n",
    "MODEL_SAVE_DIR = 'saved_models_B'\n",
    "\n",
    "# Vocabulary & Special Tokens\n",
    "VOCAB_FREQ_THRESHOLD_PERCENT = 1.0 # Minimum % of training documents a token must appear in\n",
    "PAD_TOKEN = '<pad>'\n",
    "UNK_TOKEN = '<unk>'\n",
    "BOS_TOKEN = '<bos>'\n",
    "EOS_TOKEN = '<eos>'\n",
    "\n",
    "# Model Hyperparameters (Tune these using validation set)\n",
    "HIDDEN_DIM = 300\n",
    "EMBED_DIM = 300 #GloVe dimension\n",
    "NUM_LAYERS_ENC = 1 # For basic EncoderRNN \n",
    "NUM_LAYERS_DEC = 1 # For basic DecoderRNN\n",
    "DROPOUT = 0.3\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 32 # Adjust based on GPU memory\n",
    "NUM_EPOCHS = 5 \n",
    "CLIP_GRAD = 1.0 # Gradient clipping threshold\n",
    "\n",
    "# Generation Hyperparameters\n",
    "MAX_NEW_TOKENS = 30 # Max length of generated titles\n",
    "BEAM_WIDTH = 4 # For beam search evaluation\n",
    "\n",
    "# Other Settings\n",
    "SEED = 42\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "REPORT_INTERVAL = 50 # Print training progress every N batches\n",
    "\n",
    "# Create model save directory\n",
    "os.makedirs(MODEL_SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# --- Set Random Seeds ---\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# --- Vocabulary Loading/Building ---\n",
    "# If you didn't save vocab in Part A, build it here based on processed train data\n",
    "\n",
    "def build_or_load_vocab(train_filepath, threshold_percent, save_path=VOCAB_FILE):\n",
    "    if os.path.exists(save_path):\n",
    "        print(f\"Loading vocabulary from {save_path}...\")\n",
    "        try:\n",
    "            with open(save_path, 'rb') as f:\n",
    "                vocab_data = pickle.load(f)\n",
    "            word2idx = vocab_data['word2idx']\n",
    "            idx2word = vocab_data['idx2word']\n",
    "            print(f\"Loaded vocabulary with {len(word2idx)} tokens.\")\n",
    "            return word2idx, idx2word\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading vocab file {save_path}: {e}. Rebuilding...\")\n",
    "\n",
    "    print(\"Building vocabulary from training data...\")\n",
    "    df_train = pd.read_csv(train_filepath)\n",
    "    df_train.fillna('', inplace=True) # Handle potential NaNs\n",
    "\n",
    "    # Count token occurrences across documents (not just total frequency)\n",
    "    doc_counts = Counter()\n",
    "    all_tokens = set()\n",
    "    for text in df_train['processed_text']:\n",
    "        tokens = set(text.split())\n",
    "        doc_counts.update(tokens)\n",
    "        all_tokens.update(tokens)\n",
    "    for title in df_train['processed_title']:\n",
    "        tokens = set(title.split())\n",
    "        doc_counts.update(tokens)\n",
    "        all_tokens.update(tokens)\n",
    "\n",
    "    num_docs = len(df_train)\n",
    "    threshold_count = (threshold_percent / 100.0) * num_docs\n",
    "\n",
    "    vocab = {PAD_TOKEN: 0, UNK_TOKEN: 1, BOS_TOKEN: 2, EOS_TOKEN: 3}\n",
    "    idx_counter = 4\n",
    "    for token, count in doc_counts.items():\n",
    "        if count >= threshold_count:\n",
    "            vocab[token] = idx_counter\n",
    "            idx_counter += 1\n",
    "\n",
    "    word2idx = vocab\n",
    "    idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "    print(f\"Built vocabulary with {len(word2idx)} tokens (appeared in >= {threshold_percent:.2f}% of docs).\")\n",
    "\n",
    "    # Save the vocabulary\n",
    "    try:\n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "        with open(save_path, 'wb') as f:\n",
    "            pickle.dump({'word2idx': word2idx, 'idx2word': idx2word}, f)\n",
    "        print(f\"Vocabulary saved to {save_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving vocabulary: {e}\")\n",
    "\n",
    "    return word2idx, idx2word\n",
    "\n",
    "# --- GloVe Embedding Loading ---\n",
    "\n",
    "def load_glove_embeddings(glove_path, word2idx, embed_dim):\n",
    "    if not os.path.exists(glove_path):\n",
    "        print(f\"GloVe file not found at {glove_path}. Cannot load embeddings.\")\n",
    "        return None\n",
    "\n",
    "    print(f\"Loading GloVe embeddings from {glove_path}...\")\n",
    "    embeddings_index = {}\n",
    "    try:\n",
    "        with open(glove_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                values = line.split()\n",
    "                word = values[0]\n",
    "                vector = np.asarray(values[1:], dtype='float32')\n",
    "                if len(vector) == embed_dim: # Ensure dimension matches\n",
    "                     embeddings_index[word] = vector\n",
    "                # else:\n",
    "                #     print(f\"Warning: Skipping word '{word}' with unexpected dimension {len(vector)} (expected {embed_dim})\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading GloVe file: {e}\")\n",
    "        return None\n",
    "\n",
    "    print(f\"Loaded {len(embeddings_index)} word vectors.\")\n",
    "\n",
    "    vocab_size = len(word2idx)\n",
    "    embedding_matrix = np.zeros((vocab_size, embed_dim))\n",
    "    hits = 0\n",
    "    misses = 0\n",
    "\n",
    "    for word, i in word2idx.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # Words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            hits += 1\n",
    "        else:\n",
    "            # Optional: Initialize OOV words randomly (e.g., np.random.randn)\n",
    "            # Or keep them as zeros\n",
    "            misses += 1\n",
    "\n",
    "    print(f\"Converted GloVe embeddings to matrix: {hits} words found, {misses} words not found in GloVe.\")\n",
    "    return torch.tensor(embedding_matrix, dtype=torch.float)\n",
    "\n",
    "\n",
    "# --- Dataset and DataLoader ---\n",
    "\n",
    "class WikipediaDataset(Dataset):\n",
    "    def __init__(self, filepath, word2idx, max_len_text=None, max_len_title=None):\n",
    "        self.df = pd.read_csv(filepath)\n",
    "        self.df.fillna('', inplace=True) # Ensure no NaNs\n",
    "\n",
    "        # Filter out rows that became empty during preprocessing in Part A again, just in case\n",
    "        self.df = self.df[(self.df['processed_text'].str.strip() != \"\") & (self.df['processed_title'].str.strip() != \"\")]\n",
    "        self.df = self.df.reset_index(drop=True)\n",
    "\n",
    "        self.texts = self.df['processed_text'].tolist()\n",
    "        self.titles = self.df['processed_title'].tolist()\n",
    "        self.word2idx = word2idx\n",
    "        self.unk_idx = word2idx.get(UNK_TOKEN, 1) # Default UNK index\n",
    "        self.bos_idx = word2idx.get(BOS_TOKEN, 2)\n",
    "        self.eos_idx = word2idx.get(EOS_TOKEN, 3)\n",
    "\n",
    "        # Optional: Store max lengths if needed, though padding handles this\n",
    "        self.max_len_text = max_len_text\n",
    "        self.max_len_title = max_len_title\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        title = self.titles[idx]\n",
    "\n",
    "        # Convert text to indices\n",
    "        text_tokens = text.split()\n",
    "        text_indices = [self.word2idx.get(token, self.unk_idx) for token in text_tokens]\n",
    "        # Optional: Truncate if max_len_text is set\n",
    "\n",
    "        # Convert title to indices, adding BOS/EOS\n",
    "        title_tokens = title.split()\n",
    "        title_indices = [self.bos_idx] + [self.word2idx.get(token, self.unk_idx) for token in title_tokens] + [self.eos_idx]\n",
    "        # Optional: Truncate if max_len_title is set\n",
    "\n",
    "        return torch.tensor(text_indices, dtype=torch.long), torch.tensor(title_indices, dtype=torch.long)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Pads sequences within a batch.\n",
    "    Args:\n",
    "        batch: A list of tuples (text_indices, title_indices)\n",
    "    Returns:\n",
    "        texts_padded: Tensor of padded text sequences\n",
    "        text_lengths: Tensor of original text lengths\n",
    "        titles_padded: Tensor of padded title sequences\n",
    "        title_lengths: Tensor of original title lengths\n",
    "    \"\"\"\n",
    "    texts, titles = zip(*batch)\n",
    "\n",
    "    # Get lengths before padding\n",
    "    text_lengths = torch.tensor([len(t) for t in texts], dtype=torch.long)\n",
    "    title_lengths = torch.tensor([len(t) for t in titles], dtype=torch.long)\n",
    "\n",
    "    # Pad sequences\n",
    "    # batch_first=True is important for GRU/LSTM input shape\n",
    "    texts_padded = pad_sequence(texts, batch_first=True, padding_value=word2idx[PAD_TOKEN])\n",
    "    titles_padded = pad_sequence(titles, batch_first=True, padding_value=word2idx[PAD_TOKEN])\n",
    "\n",
    "    return texts_padded, text_lengths, titles_padded, title_lengths\n",
    "\n",
    "# --- Model Definitions ---\n",
    "\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers, dropout, bidirectional=True):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=word2idx[PAD_TOKEN])\n",
    "        self.rnn = nn.GRU(embed_dim, hidden_dim, num_layers=num_layers,\n",
    "                          bidirectional=bidirectional, dropout=dropout if num_layers > 1 else 0,\n",
    "                          batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.num_directions = 2 if bidirectional else 1\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Optional: Linear layer to project hidden states if needed (e.g., if decoder has different dim)\n",
    "        # self.fc = nn.Linear(hidden_dim * self.num_directions, hidden_dim)\n",
    "\n",
    "    def forward(self, src, src_len):\n",
    "        # src: [batch_size, seq_len]\n",
    "        # src_len: [batch_size]\n",
    "\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        # embedded: [batch_size, seq_len, embed_dim]\n",
    "\n",
    "        # Pack sequence to handle padding efficiently\n",
    "        packed_embedded = pack_padded_sequence(embedded, src_len.cpu(), batch_first=True, enforce_sorted=False)\n",
    "\n",
    "        packed_outputs, hidden = self.rnn(packed_embedded)\n",
    "        # packed_outputs: packed sequence\n",
    "        # hidden: [num_layers * num_directions, batch_size, hidden_dim]\n",
    "\n",
    "        # Unpack sequence (optional, useful if you need intermediate outputs)\n",
    "        outputs, _ = pad_packed_sequence(packed_outputs, batch_first=True)\n",
    "        # outputs: [batch_size, seq_len, hidden_dim * num_directions]\n",
    "\n",
    "        # Process hidden state for decoder initialization\n",
    "        # If bidirectional, hidden is [2*num_layers, batch, hid_dim]. Need to combine directions.\n",
    "        # Often, the forward and backward hidden states are summed or concatenated.\n",
    "        # For a single layer BiGRU, hidden is [2, batch, hid_dim].\n",
    "        # Concatenate forward and backward final hidden states\n",
    "        if self.rnn.bidirectional:\n",
    "             # Reshape hidden: [num_layers, num_directions, batch, hidden_dim]\n",
    "             hidden = hidden.view(self.rnn.num_layers, self.num_directions, -1, self.hidden_dim)\n",
    "             # Concatenate the directions for the last layer:\n",
    "             # Take hidden state of the last layer: hidden[-1] -> [num_directions, batch, hidden_dim]\n",
    "             # Concatenate along dim 2 -> [batch, hidden_dim * 2] -> need [batch, hidden_dim] or similar for decoder\n",
    "             # Option 1: Sum forward and backward of the last layer\n",
    "             # hidden = hidden[-1, 0, :, :] + hidden[-1, 1, :, :] # -> [batch, hidden_dim]\n",
    "             # Option 2: Pass through a linear layer to reduce dimension\n",
    "             # hidden_concat = torch.cat((hidden[-1, 0, :, :], hidden[-1, 1, :, :]), dim=1) # -> [batch, hidden_dim*2]\n",
    "             # hidden = torch.tanh(self.fc(hidden_concat)) # -> [batch, hidden_dim] - Need to adjust for multi-layer decoders\n",
    "             # Option 3: Reshape for multi-layer decoder (sum directions per layer)\n",
    "             # Reshape hidden -> [num_layers, batch_size, hidden_dim * num_directions]\n",
    "             hidden = hidden.permute(0, 2, 1, 3).contiguous() # -> [layers, batch, directions, dim]\n",
    "             # Sum directions: requires decoder hidden_dim to match encoder hidden_dim\n",
    "             hidden = hidden.sum(dim=2) # -> [layers, batch, dim]\n",
    "\n",
    "        # Ensure hidden has shape [num_decoder_layers, batch_size, decoder_hidden_dim]\n",
    "        # This depends on the decoder structure. For a basic 1-layer decoder matching encoder hid_dim:\n",
    "        # If encoder is multi-layer bidirectional, take the summed hidden state of the *last* encoder layer.\n",
    "        # If encoder is 1-layer bidirectional, sum is already [1, batch, dim] assuming decoder is 1-layer.\n",
    "        if hidden.shape[0] > NUM_LAYERS_DEC: # If encoder layers > decoder layers\n",
    "            hidden = hidden[-NUM_LAYERS_DEC:] # Take only the last N layers\n",
    "\n",
    "\n",
    "        # outputs: [batch_size, seq_len, hidden_dim * num_directions]\n",
    "        # hidden: [num_decoder_layers, batch_size, decoder_hidden_dim] - Processed state for decoder\n",
    "        return outputs, hidden\n",
    "\n",
    "\n",
    "class HierEncoderRNN(nn.Module):\n",
    "     # TODO: Implement Hierarchical Encoder (Part B2, Task 5)\n",
    "     # Needs sentence splitting, word-level GRU, sentence aggregation (e.g., average), sentence-level GRU.\n",
    "     # Requires careful handling of padding at both levels.\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers_word, num_layers_sent, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=word2idx[PAD_TOKEN])\n",
    "        self.word_rnn = nn.GRU(embed_dim, hidden_dim, num_layers=num_layers_word,\n",
    "                               bidirectional=True, dropout=dropout if num_layers_word > 1 else 0,\n",
    "                               batch_first=True)\n",
    "        self.sent_rnn = nn.GRU(hidden_dim * 2, hidden_dim, num_layers=num_layers_sent, # Input is concat hidden states\n",
    "                               bidirectional=True, dropout=dropout if num_layers_sent > 1 else 0,\n",
    "                               batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        # Layer to potentially transform final hidden state for decoder\n",
    "        self.fc_hidden = nn.Linear(hidden_dim * 2 * num_layers_sent, hidden_dim * NUM_LAYERS_DEC) # Match decoder layers/dim\n",
    "\n",
    "    def forward(self, src, src_len):\n",
    "        # src: [batch_size, max_seq_len] - Raw token indices for the whole article\n",
    "        # src_len: [batch_size] - Original lengths of articles\n",
    "\n",
    "        # This implementation is complex due to varying sentence/word counts.\n",
    "        # A common approach involves processing sentence by sentence or using libraries\n",
    "        # that handle nested padding (like AllenNLP or careful manual padding/masking).\n",
    "\n",
    "        # Placeholder: This needs a proper implementation using sentence tokenization\n",
    "        # and handling the nested structure. The standard EncoderRNN is simpler.\n",
    "        # For now, let's return dummy values or raise NotImplementedError\n",
    "        raise NotImplementedError(\"Hierarchical Encoder needs full implementation\")\n",
    "\n",
    "        # A simplified view (conceptual):\n",
    "        # 1. Pad articles to max_article_len\n",
    "        # 2. Split each article into sentences (pad sentences to max_sent_len, pad num_sentences to max_num_sent)\n",
    "        # 3. Process words in each sentence with word_rnn\n",
    "        # 4. Aggregate word outputs per sentence (e.g., take last hidden state, or average outputs)\n",
    "        # 5. Process aggregated sentence representations with sent_rnn\n",
    "        # 6. Extract final hidden state for the decoder\n",
    "\n",
    "        # Returning dummy values matching expected shapes\n",
    "        # batch_size = src.shape[0]\n",
    "        # seq_len = src.shape[1]\n",
    "        # outputs = torch.zeros(batch_size, seq_len, self.hidden_dim * 2).to(src.device) # Dummy outputs\n",
    "        # hidden = torch.zeros(NUM_LAYERS_DEC, batch_size, self.hidden_dim).to(src.device) # Dummy hidden\n",
    "        # return outputs, hidden\n",
    "\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=word2idx[PAD_TOKEN])\n",
    "        self.rnn = nn.GRU(embed_dim, hidden_dim, num_layers=num_layers,\n",
    "                           dropout=dropout if num_layers > 1 else 0, batch_first=True)\n",
    "        self.fc_out = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input_token, hidden):\n",
    "        # input_token: [batch_size] - current input token indices\n",
    "        # hidden: [num_layers, batch_size, hidden_dim] - previous hidden state\n",
    "\n",
    "        # Add sequence length dimension: [batch_size] -> [batch_size, 1]\n",
    "        input_token = input_token.unsqueeze(1)\n",
    "\n",
    "        embedded = self.dropout(self.embedding(input_token))\n",
    "        # embedded: [batch_size, 1, embed_dim]\n",
    "\n",
    "        output, hidden = self.rnn(embedded, hidden)\n",
    "        # output: [batch_size, 1, hidden_dim]\n",
    "        # hidden: [num_layers, batch_size, hidden_dim]\n",
    "\n",
    "        # Predict next token\n",
    "        prediction = self.fc_out(output.squeeze(1))\n",
    "        # prediction: [batch_size, vocab_size]\n",
    "\n",
    "        return prediction, hidden\n",
    "\n",
    "\n",
    "class Decoder2RNN(nn.Module):\n",
    "    # TODO: Implement Decoder with 2 GRUs (Part B2, Task 6)\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=word2idx[PAD_TOKEN])\n",
    "        # Two GRU layers\n",
    "        self.rnn1 = nn.GRU(embed_dim, hidden_dim, num_layers=1, batch_first=True) # First layer\n",
    "        self.rnn2 = nn.GRU(hidden_dim, hidden_dim, num_layers=1, batch_first=True) # Second layer\n",
    "        self.fc_out = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input_token, hidden):\n",
    "        # input_token: [batch_size]\n",
    "        # hidden: Tuple of hidden states (hidden1, hidden2) or a single tensor to split?\n",
    "        # Assuming hidden is [num_layers=2, batch_size, hidden_dim] matching encoder output\n",
    "        # If encoder outputs [1, batch, dim], need to duplicate/adapt for 2 layers\n",
    "\n",
    "        if hidden.shape[0] != 2:\n",
    "             # Handle mismatch if encoder provides only 1 layer state\n",
    "             # Option: duplicate encoder's last layer state\n",
    "             if hidden.shape[0] == 1:\n",
    "                 hidden = hidden.repeat(2, 1, 1)\n",
    "             else: # Or just take first 2 layers if encoder provided more\n",
    "                 hidden = hidden[:2]\n",
    "\n",
    "        # Split hidden state for each GRU layer\n",
    "        hidden1 = hidden[0:1] # Shape [1, batch, dim]\n",
    "        hidden2 = hidden[1:2] # Shape [1, batch, dim]\n",
    "\n",
    "        input_token = input_token.unsqueeze(1)\n",
    "        # input_token: [batch_size, 1]\n",
    "\n",
    "        embedded = self.dropout(self.embedding(input_token))\n",
    "        # embedded: [batch_size, 1, embed_dim]\n",
    "\n",
    "        output1, hidden1_out = self.rnn1(embedded, hidden1)\n",
    "        # output1: [batch_size, 1, hidden_dim]\n",
    "        # hidden1_out: [1, batch_size, hidden_dim]\n",
    "\n",
    "        output2, hidden2_out = self.rnn2(output1, hidden2) # Feed output of rnn1 to rnn2\n",
    "        # output2: [batch_size, 1, hidden_dim]\n",
    "        # hidden2_out: [1, batch_size, hidden_dim]\n",
    "\n",
    "        prediction = self.fc_out(output2.squeeze(1))\n",
    "        # prediction: [batch_size, vocab_size]\n",
    "\n",
    "        # Combine hidden states for next step\n",
    "        updated_hidden = torch.cat((hidden1_out, hidden2_out), dim=0) # Shape [2, batch, dim]\n",
    "\n",
    "        return prediction, updated_hidden\n",
    "\n",
    "\n",
    "class Seq2seqRNN(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device, bos_idx, eos_idx, pad_idx):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        self.bos_idx = bos_idx\n",
    "        self.eos_idx = eos_idx\n",
    "        self.pad_idx = pad_idx\n",
    "\n",
    "    def load_embeddings(self, embedding_matrix, freeze=False):\n",
    "        \"\"\"Loads pre-trained embeddings into the encoder.\"\"\"\n",
    "        if embedding_matrix is None:\n",
    "            print(\"Embedding matrix is None, cannot load.\")\n",
    "            return\n",
    "        if hasattr(self.encoder, 'embedding'):\n",
    "            self.encoder.embedding.weight.data.copy_(embedding_matrix)\n",
    "            self.encoder.embedding.weight.requires_grad = not freeze\n",
    "            print(f\"Loaded embeddings into encoder. Freeze: {freeze}\")\n",
    "        else:\n",
    "            print(\"Encoder does not have an 'embedding' attribute to load into.\")\n",
    "\n",
    "    def forward(self, src, src_len, trg):\n",
    "        # src: [batch_size, src_len]\n",
    "        # src_len: [batch_size]\n",
    "        # trg: [batch_size, trg_len]\n",
    "\n",
    "        batch_size = trg.shape[0]\n",
    "        trg_len = trg.shape[1]\n",
    "        trg_vocab_size = self.decoder.vocab_size\n",
    "\n",
    "        # Tensor to store decoder outputs\n",
    "        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)\n",
    "\n",
    "        # Encode source sequence\n",
    "        encoder_outputs, hidden = self.encoder(src, src_len)\n",
    "        # encoder_outputs: potentially used for attention later\n",
    "        # hidden: final hidden state(s) to initialize decoder\n",
    "\n",
    "        # Decoder input starts with <bos> token\n",
    "        input_token = trg[:, 0] # [batch_size]\n",
    "\n",
    "        # Teacher Forcing: Feed target sequence token-by-token\n",
    "        for t in range(1, trg_len): # Start from 1, predict token t based on input t-1\n",
    "            output, hidden = self.decoder(input_token, hidden)\n",
    "\n",
    "            # Store prediction\n",
    "            outputs[:, t, :] = output\n",
    "\n",
    "            # Get next input token from target sequence\n",
    "            input_token = trg[:, t]\n",
    "\n",
    "        # outputs: [batch_size, trg_len, vocab_size] - contains logits starting from index 1\n",
    "        return outputs\n",
    "\n",
    "    def generate(self, src, src_len, max_len, beam_width=1):\n",
    "        \"\"\"Generate titles for source sequences.\"\"\"\n",
    "        if beam_width == 1:\n",
    "            return self._generate_greedy(src, src_len, max_len)\n",
    "        else:\n",
    "            return self._generate_beam_search(src, src_len, max_len, beam_width)\n",
    "\n",
    "    def _generate_greedy(self, src, src_len, max_len):\n",
    "        self.eval() # Set to evaluation mode\n",
    "        batch_size = src.shape[0]\n",
    "        generated_sequences = torch.zeros(batch_size, max_len, dtype=torch.long).fill_(self.pad_idx).to(self.device)\n",
    "        generated_sequences[:, 0] = self.bos_idx # Start with <bos>\n",
    "\n",
    "        with torch.no_grad():\n",
    "            _, hidden = self.encoder(src, src_len)\n",
    "            input_token = torch.tensor([self.bos_idx] * batch_size).to(self.device) # [batch_size]\n",
    "\n",
    "            # Keep track of which sequences have finished (hit <eos>)\n",
    "            finished = torch.zeros(batch_size, dtype=torch.bool).to(self.device)\n",
    "\n",
    "            for t in range(1, max_len):\n",
    "                output, hidden = self.decoder(input_token, hidden) # output: [batch_size, vocab_size]\n",
    "\n",
    "                # Get the token with the highest probability (greedy)\n",
    "                top1 = output.argmax(1) # [batch_size]\n",
    "\n",
    "                # Update generated sequences for non-finished ones\n",
    "                generated_sequences[~finished, t] = top1[~finished]\n",
    "\n",
    "                # Update input token for next step (only for non-finished)\n",
    "                input_token = top1\n",
    "\n",
    "                # Update finished mask\n",
    "                just_finished = (top1 == self.eos_idx)\n",
    "                finished |= just_finished\n",
    "\n",
    "                # Stop if all sequences have finished\n",
    "                if finished.all():\n",
    "                    break\n",
    "\n",
    "        # Return sequences (excluding the initial <bos> potentially)\n",
    "        return generated_sequences\n",
    "\n",
    "    def _generate_beam_search(self, src, src_len, max_len, beam_width):\n",
    "        # TODO: Implement Beam Search (Part B2, Task 7)\n",
    "        self.eval()\n",
    "        batch_size = src.shape[0]\n",
    "        # This is significantly more complex than greedy search.\n",
    "        # It involves maintaining `beam_width` candidate sequences per batch item,\n",
    "        # along with their cumulative log-probabilities.\n",
    "        # Need data structures to manage beams, scores, hidden states per beam.\n",
    "\n",
    "        # Placeholder - Returning greedy results for now\n",
    "        print(\"Warning: Beam search not implemented yet. Returning greedy results.\")\n",
    "        return self._generate_greedy(src, src_len, max_len)\n",
    "\n",
    "        # Key steps for beam search:\n",
    "        # 1. Encode source: Get initial hidden state.\n",
    "        # 2. Initialize beams: Start with <bos> token, score 0. Maintain k beams.\n",
    "        # 3. Loop t = 1 to max_len:\n",
    "        #    a. For each beam candidate from previous step:\n",
    "        #       i. Feed its last token and hidden state to decoder. Get output logits.\n",
    "        #       ii. Calculate log probabilities (log_softmax).\n",
    "        #       iii. Get top-k next token candidates and their probabilities.\n",
    "        #       iv. Calculate cumulative scores for new potential beams (prev_score + new_log_prob).\n",
    "        #    b. Collect all potential new beams across all current beams.\n",
    "        #    c. Select top-k overall beams based on cumulative scores. Prune the rest.\n",
    "        #    d. Handle finished sequences (<eos>): Store completed beams separately. Continue expanding others.\n",
    "        # 4. Return the best completed beam(s).\n",
    "\n",
    "\n",
    "# --- Training and Evaluation Functions ---\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, criterion, clip, device):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    num_batches = len(dataloader)\n",
    "\n",
    "    for i, (src, src_len, trg, trg_len) in enumerate(dataloader):\n",
    "        src, src_len, trg = src.to(device), src_len.to(device), trg.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Shape: [batch_size, trg_len, vocab_size]\n",
    "        output = model(src, src_len, trg)\n",
    "\n",
    "        # Reshape for loss calculation:\n",
    "        # output: [(batch_size * (trg_len - 1)), vocab_size]\n",
    "        # trg: [batch_size * (trg_len - 1)]\n",
    "        # Ignore the first token (<bos>) in target and output\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output[:, 1:, :].contiguous().view(-1, output_dim)\n",
    "        trg = trg[:, 1:].contiguous().view(-1)\n",
    "\n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        if (i + 1) % REPORT_INTERVAL == 0:\n",
    "            print(f'Batch {i+1}/{num_batches} | Loss: {loss.item():.4f}')\n",
    "\n",
    "    return epoch_loss / num_batches\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for src, src_len, trg, trg_len in dataloader:\n",
    "            src, src_len, trg = src.to(device), src_len.to(device), trg.to(device)\n",
    "\n",
    "            output = model(src, src_len, trg) # [batch, trg_len, vocab_size]\n",
    "\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output[:, 1:, :].contiguous().view(-1, output_dim)\n",
    "            trg = trg[:, 1:].contiguous().view(-1)\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(dataloader)\n",
    "\n",
    "def predict_and_evaluate_rouge(model, dataloader, idx2word, device, generation_params):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_references = []\n",
    "    pad_idx = word2idx[PAD_TOKEN]\n",
    "    eos_idx = word2idx[EOS_TOKEN]\n",
    "    bos_idx = word2idx[BOS_TOKEN]\n",
    "\n",
    "    print(f\"Generating predictions with params: {generation_params}\")\n",
    "    with torch.no_grad():\n",
    "        for src, src_len, trg, trg_len in dataloader: # Use test loader here\n",
    "            src, src_len = src.to(device), src_len.to(device)\n",
    "\n",
    "            # Generate predictions (indices)\n",
    "            # Shape: [batch_size, max_len]\n",
    "            generated_indices = model.generate(src, src_len, **generation_params)\n",
    "\n",
    "            # Convert indices to text\n",
    "            for i in range(generated_indices.shape[0]):\n",
    "                # Predicted sequence\n",
    "                pred_seq = []\n",
    "                for idx in generated_indices[i].tolist():\n",
    "                    if idx == bos_idx: continue # Skip <bos>\n",
    "                    if idx == eos_idx: break # Stop at <eos>\n",
    "                    if idx == pad_idx: continue # Skip <pad>\n",
    "                    pred_seq.append(idx2word.get(idx, UNK_TOKEN))\n",
    "                all_predictions.append(\" \".join(pred_seq))\n",
    "\n",
    "                # Reference sequence (from the batch)\n",
    "                ref_seq = []\n",
    "                # trg is padded, use trg_len to get original length\n",
    "                # Correct indexing needed - trg includes BOS/EOS\n",
    "                actual_trg_len = trg_len[i].item()\n",
    "                for idx in trg[i, 1:actual_trg_len-1].tolist(): # Exclude BOS and EOS\n",
    "                     if idx == pad_idx: break # Should not happen if length is correct\n",
    "                     ref_seq.append(idx2word.get(idx, UNK_TOKEN))\n",
    "                all_references.append(\" \".join(ref_seq))\n",
    "\n",
    "    print(f\"Generated {len(all_predictions)} predictions.\")\n",
    "    # Example outputs:\n",
    "    if len(all_predictions) > 0 and len(all_references) > 0:\n",
    "        print(\"Example Prediction :\", all_predictions[0])\n",
    "        print(\"Example Reference  :\", all_references[0])\n",
    "\n",
    "    # Calculate ROUGE scores\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    total_scores = {'rouge1': 0.0, 'rouge2': 0.0, 'rougeL': 0.0}\n",
    "    num_samples = len(all_predictions)\n",
    "\n",
    "    if num_samples == 0:\n",
    "        return {'rouge1': 0, 'rouge2': 0, 'rougeL': 0}\n",
    "\n",
    "    for pred, ref in zip(all_predictions, all_references):\n",
    "        scores = scorer.score(ref, pred)\n",
    "        total_scores['rouge1'] += scores['rouge1'].fmeasure\n",
    "        total_scores['rouge2'] += scores['rouge2'].fmeasure\n",
    "        total_scores['rougeL'] += scores['rougeL'].fmeasure\n",
    "\n",
    "    avg_scores = {metric: score / num_samples for metric, score in total_scores.items()}\n",
    "    print(f\"ROUGE Scores: R1: {avg_scores['rouge1']:.4f}, R2: {avg_scores['rouge2']:.4f}, RL: {avg_scores['rougeL']:.4f}\")\n",
    "    return avg_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-13T12:06:07.318486Z",
     "iopub.status.busy": "2025-04-13T12:06:07.318248Z",
     "iopub.status.idle": "2025-04-13T12:57:36.003905Z",
     "shell.execute_reply": "2025-04-13T12:57:36.003085Z",
     "shell.execute_reply.started": "2025-04-13T12:06:07.318466Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main execution started. Using device: cuda\n",
      "NOTE: Experiments will run sequentially in this script.\n",
      "      For parallel execution on Kaggle, create separate notebooks for each part and run them concurrently.\n",
      "Building vocabulary from training data...\n",
      "Built vocabulary with 8274 tokens (appeared in >= 1.00% of docs).\n",
      "Error saving vocabulary: [Errno 30] Read-only file system: '/kaggle/input/processedassignment2datanlp/vocabulary.pkl'\n",
      "Time taken for vocabulary: 5.63 seconds\n",
      "Time taken for data loading: 1.43 seconds\n",
      "\n",
      "--- Running Experiment: BasicRNN ---\n",
      "Config: Enc=basic, Dec=basic, GloVe=False, Beam=1\n",
      "Starting training...\n",
      "Batch 50/414 | Loss: 3.6607\n",
      "Batch 100/414 | Loss: 2.5882\n",
      "Batch 150/414 | Loss: 2.5471\n",
      "Batch 200/414 | Loss: 2.8385\n",
      "Batch 250/414 | Loss: 2.9977\n",
      "Batch 300/414 | Loss: 3.0158\n",
      "Batch 350/414 | Loss: 2.0967\n",
      "Batch 400/414 | Loss: 2.6329\n",
      "Epoch 1/5 | Time: 3m 22s | Saving Best Model...\n",
      "\tTrain Loss: 2.9897 | Val. Loss: 2.1161\n",
      "Batch 50/414 | Loss: 1.8913\n",
      "Batch 100/414 | Loss: 1.4383\n",
      "Batch 150/414 | Loss: 1.2607\n",
      "Batch 200/414 | Loss: 1.7885\n",
      "Batch 250/414 | Loss: 1.8051\n",
      "Batch 300/414 | Loss: 0.8388\n",
      "Batch 350/414 | Loss: 1.3891\n",
      "Batch 400/414 | Loss: 1.0894\n",
      "Epoch 2/5 | Time: 3m 20s | Saving Best Model...\n",
      "\tTrain Loss: 1.6457 | Val. Loss: 1.7953\n",
      "Batch 50/414 | Loss: 1.3093\n",
      "Batch 100/414 | Loss: 1.0504\n",
      "Batch 150/414 | Loss: 1.0227\n",
      "Batch 200/414 | Loss: 0.9345\n",
      "Batch 250/414 | Loss: 0.9902\n",
      "Batch 300/414 | Loss: 1.3495\n",
      "Batch 350/414 | Loss: 0.7925\n",
      "Batch 400/414 | Loss: 0.9502\n",
      "Epoch 3/5 | Time: 3m 21s | Saving Best Model...\n",
      "\tTrain Loss: 1.0099 | Val. Loss: 1.6586\n",
      "Batch 50/414 | Loss: 0.5624\n",
      "Batch 100/414 | Loss: 0.4622\n",
      "Batch 150/414 | Loss: 0.5674\n",
      "Batch 200/414 | Loss: 0.5107\n",
      "Batch 250/414 | Loss: 0.5632\n",
      "Batch 300/414 | Loss: 0.5006\n",
      "Batch 350/414 | Loss: 0.3245\n",
      "Batch 400/414 | Loss: 0.5465\n",
      "Epoch 4/5 | Time: 3m 24s | Saving Best Model...\n",
      "\tTrain Loss: 0.5756 | Val. Loss: 1.6084\n",
      "Batch 50/414 | Loss: 0.3470\n",
      "Batch 100/414 | Loss: 0.3154\n",
      "Batch 150/414 | Loss: 0.2354\n",
      "Batch 200/414 | Loss: 0.2784\n",
      "Batch 250/414 | Loss: 0.3324\n",
      "Batch 300/414 | Loss: 0.2916\n",
      "Batch 350/414 | Loss: 0.4392\n",
      "Batch 400/414 | Loss: 0.1837\n",
      "Epoch 5/5 | Time: 3m 23s | Saving Best Model...\n",
      "\tTrain Loss: 0.3102 | Val. Loss: 1.5873\n",
      "Training finished. Time taken: 1010.85 seconds\n",
      "Loading best trained model from saved_models_B/BasicRNN_model.pt\n",
      "Evaluating with Greedy Search...\n",
      "Generating predictions with params: {'max_len': 30, 'beam_width': 1}\n",
      "Generated 100 predictions.\n",
      "Example Prediction : <unk>\n",
      "Example Reference  : <unk>\n",
      "ROUGE Scores: R1: 0.7072, R2: 0.4043, RL: 0.7072\n",
      "--- Experiment BasicRNN Finished --- Time: 1013.01s | ROUGE-L: 0.7072\n",
      "\n",
      "--- Running Experiment: BasicRNN_GloVe ---\n",
      "Config: Enc=basic, Dec=basic, GloVe=True, Beam=1\n",
      "Loading GloVe embeddings from /kaggle/input/glove-embeddings/glove.6B.300d.txt...\n",
      "Loaded 400000 word vectors.\n",
      "Converted GloVe embeddings to matrix: 8159 words found, 115 words not found in GloVe.\n",
      "Loaded embeddings into encoder. Freeze: False\n",
      "Starting training...\n",
      "Batch 50/414 | Loss: 3.8280\n",
      "Batch 100/414 | Loss: 3.2086\n",
      "Batch 150/414 | Loss: 4.0323\n",
      "Batch 200/414 | Loss: 2.8022\n",
      "Batch 250/414 | Loss: 2.8447\n",
      "Batch 300/414 | Loss: 3.1575\n",
      "Batch 350/414 | Loss: 2.9384\n",
      "Batch 400/414 | Loss: 3.2668\n",
      "Epoch 1/5 | Time: 3m 23s | Saving Best Model...\n",
      "\tTrain Loss: 3.2310 | Val. Loss: 2.3635\n",
      "Batch 50/414 | Loss: 2.0084\n",
      "Batch 100/414 | Loss: 1.9006\n",
      "Batch 150/414 | Loss: 2.1663\n",
      "Batch 200/414 | Loss: 1.5802\n",
      "Batch 250/414 | Loss: 1.7519\n",
      "Batch 300/414 | Loss: 2.0576\n",
      "Batch 350/414 | Loss: 2.2108\n",
      "Batch 400/414 | Loss: 1.4239\n",
      "Epoch 2/5 | Time: 3m 23s | Saving Best Model...\n",
      "\tTrain Loss: 1.9253 | Val. Loss: 1.9216\n",
      "Batch 50/414 | Loss: 2.0136\n",
      "Batch 100/414 | Loss: 1.2835\n",
      "Batch 150/414 | Loss: 1.3160\n",
      "Batch 200/414 | Loss: 1.0633\n",
      "Batch 250/414 | Loss: 1.4591\n",
      "Batch 300/414 | Loss: 1.5692\n",
      "Batch 350/414 | Loss: 1.2622\n",
      "Batch 400/414 | Loss: 1.3028\n",
      "Epoch 3/5 | Time: 3m 22s | Saving Best Model...\n",
      "\tTrain Loss: 1.2638 | Val. Loss: 1.6966\n",
      "Batch 50/414 | Loss: 0.5095\n",
      "Batch 100/414 | Loss: 0.7317\n",
      "Batch 150/414 | Loss: 0.9789\n",
      "Batch 200/414 | Loss: 0.9634\n",
      "Batch 250/414 | Loss: 0.8292\n",
      "Batch 300/414 | Loss: 0.6298\n",
      "Batch 350/414 | Loss: 0.8315\n",
      "Batch 400/414 | Loss: 0.7587\n",
      "Epoch 4/5 | Time: 3m 21s | Saving Best Model...\n",
      "\tTrain Loss: 0.7716 | Val. Loss: 1.5843\n",
      "Batch 50/414 | Loss: 0.4222\n",
      "Batch 100/414 | Loss: 0.3452\n",
      "Batch 150/414 | Loss: 0.3646\n",
      "Batch 200/414 | Loss: 0.5455\n",
      "Batch 250/414 | Loss: 0.5053\n",
      "Batch 300/414 | Loss: 0.1770\n",
      "Batch 350/414 | Loss: 0.4602\n",
      "Batch 400/414 | Loss: 0.4943\n",
      "Epoch 5/5 | Time: 3m 23s\n",
      "\tTrain Loss: 0.4166 | Val. Loss: 1.5858\n",
      "Training finished. Time taken: 1012.37 seconds\n",
      "Loading best trained model from saved_models_B/BasicRNN_GloVe_model.pt\n",
      "Evaluating with Greedy Search...\n",
      "Generating predictions with params: {'max_len': 30, 'beam_width': 1}\n",
      "Generated 100 predictions.\n",
      "Example Prediction : <unk> <unk>\n",
      "Example Reference  : <unk>\n",
      "ROUGE Scores: R1: 0.7059, R2: 0.3733, RL: 0.7059\n",
      "--- Experiment BasicRNN_GloVe Finished --- Time: 1035.79s | ROUGE-L: 0.7059\n",
      "\n",
      "--- Running Experiment: HierEncRNN ---\n",
      "Config: Enc=hierarchical, Dec=basic, GloVe=False, Beam=1\n",
      "Skipping HierEncRNN: Hierarchical Encoder not implemented. Hierarchical Encoder needs full implementation\n",
      "\n",
      "--- Running Experiment: Decoder2RNN ---\n",
      "Config: Enc=basic, Dec=decoder2, GloVe=False, Beam=1\n",
      "Starting training...\n",
      "Batch 50/414 | Loss: 4.4937\n",
      "Batch 100/414 | Loss: 3.9985\n",
      "Batch 150/414 | Loss: 3.7788\n",
      "Batch 200/414 | Loss: 2.6546\n",
      "Batch 250/414 | Loss: 2.7183\n",
      "Batch 300/414 | Loss: 2.6186\n",
      "Batch 350/414 | Loss: 2.0912\n",
      "Batch 400/414 | Loss: 2.3591\n",
      "Epoch 1/5 | Time: 3m 25s | Saving Best Model...\n",
      "\tTrain Loss: 2.9994 | Val. Loss: 2.1303\n",
      "Batch 50/414 | Loss: 1.4687\n",
      "Batch 100/414 | Loss: 2.1994\n",
      "Batch 150/414 | Loss: 1.4344\n",
      "Batch 200/414 | Loss: 1.4013\n",
      "Batch 250/414 | Loss: 1.1717\n",
      "Batch 300/414 | Loss: 2.1261\n",
      "Batch 350/414 | Loss: 1.3870\n",
      "Batch 400/414 | Loss: 1.4100\n",
      "Epoch 2/5 | Time: 3m 28s | Saving Best Model...\n",
      "\tTrain Loss: 1.6654 | Val. Loss: 1.7556\n",
      "Batch 50/414 | Loss: 1.0665\n",
      "Batch 100/414 | Loss: 0.8677\n",
      "Batch 150/414 | Loss: 1.0725\n",
      "Batch 200/414 | Loss: 0.9639\n",
      "Batch 250/414 | Loss: 1.2598\n",
      "Batch 300/414 | Loss: 1.2446\n",
      "Batch 350/414 | Loss: 0.9018\n",
      "Batch 400/414 | Loss: 1.3385\n",
      "Epoch 3/5 | Time: 3m 25s | Saving Best Model...\n",
      "\tTrain Loss: 1.0375 | Val. Loss: 1.5854\n",
      "Batch 50/414 | Loss: 0.6172\n",
      "Batch 100/414 | Loss: 0.5649\n",
      "Batch 150/414 | Loss: 0.6900\n",
      "Batch 200/414 | Loss: 0.6048\n",
      "Batch 250/414 | Loss: 0.7457\n",
      "Batch 300/414 | Loss: 0.4686\n",
      "Batch 350/414 | Loss: 0.4971\n",
      "Batch 400/414 | Loss: 0.8257\n",
      "Epoch 4/5 | Time: 3m 26s | Saving Best Model...\n",
      "\tTrain Loss: 0.6079 | Val. Loss: 1.5494\n",
      "Batch 50/414 | Loss: 0.2036\n",
      "Batch 100/414 | Loss: 0.2140\n",
      "Batch 150/414 | Loss: 0.4479\n",
      "Batch 200/414 | Loss: 0.4384\n",
      "Batch 250/414 | Loss: 0.3389\n",
      "Batch 300/414 | Loss: 0.4512\n",
      "Batch 350/414 | Loss: 0.2637\n",
      "Batch 400/414 | Loss: 0.4889\n",
      "Epoch 5/5 | Time: 3m 27s | Saving Best Model...\n",
      "\tTrain Loss: 0.3376 | Val. Loss: 1.5070\n",
      "Training finished. Time taken: 1030.80 seconds\n",
      "Loading best trained model from saved_models_B/Decoder2RNN_model.pt\n",
      "Evaluating with Greedy Search...\n",
      "Generating predictions with params: {'max_len': 30, 'beam_width': 1}\n",
      "Generated 100 predictions.\n",
      "Example Prediction : <unk> <unk>\n",
      "Example Reference  : <unk>\n",
      "ROUGE Scores: R1: 0.7034, R2: 0.3554, RL: 0.7034\n",
      "--- Experiment Decoder2RNN Finished --- Time: 1031.74s | ROUGE-L: 0.7034\n",
      "\n",
      "--- Running Experiment: BasicRNN_Beam ---\n",
      "Config: Enc=basic, Dec=basic, GloVe=False, Beam=4\n",
      "Loading pre-trained model from: saved_models_B/BasicRNN_model.pt\n",
      "Model loaded successfully.\n",
      "Warning: Beam search not implemented yet. Returning greedy results.\n",
      "Evaluating with Beam Search (Width=4)...\n",
      "Generating predictions with params: {'max_len': 30, 'beam_width': 4}\n",
      "Warning: Beam search not implemented yet. Returning greedy results.\n",
      "Warning: Beam search not implemented yet. Returning greedy results.\n",
      "Warning: Beam search not implemented yet. Returning greedy results.\n",
      "Warning: Beam search not implemented yet. Returning greedy results.\n",
      "Generated 100 predictions.\n",
      "Example Prediction : <unk>\n",
      "Example Reference  : <unk>\n",
      "ROUGE Scores: R1: 0.7072, R2: 0.4043, RL: 0.7072\n",
      "--- Experiment BasicRNN_Beam Finished --- Time: 0.97s | ROUGE-L: 0.7072\n",
      "\n",
      "\n",
      "==================================================\n",
      " Final Results Summary (Part B)\n",
      "==================================================\n",
      "| Experiment     | Encoder   | Decoder   |    GloVe |   BeamWidth |   ROUGE-1 |   ROUGE-2 |   ROUGE-L |   Eval Time (s) |   Train Time (s) |   Total Time (s) | Status   | Reason                               |\n",
      "|:---------------|:----------|:----------|---------:|------------:|----------:|----------:|----------:|----------------:|-----------------:|-----------------:|:---------|:-------------------------------------|\n",
      "| BasicRNN       | basic     | basic     |   0.0000 |      1.0000 |    0.7072 |    0.4043 |    0.7072 |          0.9112 |        1010.8497 |        1013.0108 | nan      | nan                                  |\n",
      "| BasicRNN_GloVe | basic     | basic     |   1.0000 |      1.0000 |    0.7059 |    0.3733 |    0.7059 |          0.8353 |        1012.3669 |        1035.7931 | nan      | nan                                  |\n",
      "| HierEncRNN     | nan       | nan       | nan      |    nan      |  nan      |  nan      |  nan      |        nan      |         nan      |         nan      | skipped  | Hierarchical Encoder not implemented |\n",
      "| Decoder2RNN    | basic     | decoder2  |   0.0000 |      1.0000 |    0.7034 |    0.3554 |    0.7034 |          0.8501 |        1030.7964 |        1031.7433 | nan      | nan                                  |\n",
      "| BasicRNN_Beam  | basic     | basic     |   0.0000 |      4.0000 |    0.7072 |    0.4043 |    0.7072 |          0.8679 |           0.0000 |           0.9732 | nan      | nan                                  |\n",
      "\n",
      "Total script execution time: 3088.65 seconds (51.48 minutes)\n",
      "--- Task B Complete ---\n"
     ]
    }
   ],
   "source": [
    "# --- New Helper Function to Train/Evaluate One Configuration ---\n",
    "\n",
    "def run_experiment(\n",
    "    exp_name,\n",
    "    encoder_type,\n",
    "    decoder_type,\n",
    "    use_glove,\n",
    "    glove_freeze,\n",
    "    beam_width,\n",
    "    word2idx,\n",
    "    idx2word,\n",
    "    train_dataloader,\n",
    "    val_dataloader,\n",
    "    test_dataloader,\n",
    "    device,\n",
    "    config_params, # Dictionary containing EMBED_DIM, HIDDEN_DIM, etc.\n",
    "    skip_training=False, # New flag to skip training\n",
    "    load_model_path=None # New flag to load a specific model path\n",
    "    ):\n",
    "    \"\"\"Trains and evaluates a single model configuration.\"\"\"\n",
    "\n",
    "    exp_start_time = time.time()\n",
    "    print(f\"\\n--- Running Experiment: {exp_name} ---\")\n",
    "    print(f\"Config: Enc={encoder_type}, Dec={decoder_type}, GloVe={use_glove}, Beam={beam_width}\")\n",
    "\n",
    "    vocab_size = len(word2idx)\n",
    "    PAD_IDX = word2idx[PAD_TOKEN]\n",
    "    BOS_IDX = word2idx[BOS_TOKEN]\n",
    "    EOS_IDX = word2idx[EOS_TOKEN]\n",
    "\n",
    "    # --- Instantiate Models ---\n",
    "    # Encoder\n",
    "    if encoder_type == 'basic':\n",
    "        encoder = EncoderRNN(vocab_size, config_params['EMBED_DIM'], config_params['HIDDEN_DIM'],\n",
    "                             config_params['NUM_LAYERS_ENC'], config_params['DROPOUT'], bidirectional=True).to(device)\n",
    "    elif encoder_type == 'hierarchical':\n",
    "         try:\n",
    "             encoder = HierEncoderRNN(vocab_size, config_params['EMBED_DIM'], config_params['HIDDEN_DIM'],\n",
    "                                    num_layers_word=1, num_layers_sent=1, dropout=config_params['DROPOUT']).to(device)\n",
    "             # Trigger the check - this will raise NotImplementedError if not implemented\n",
    "             _ = encoder(torch.randint(0,10,(2,10)).to(device), torch.tensor([10,8]))\n",
    "         except NotImplementedError as e:\n",
    "             print(f\"Skipping {exp_name}: Hierarchical Encoder not implemented. {e}\")\n",
    "             return { 'status': 'skipped', 'reason': 'Hierarchical Encoder not implemented' }\n",
    "         except Exception as e: # Catch other potential errors in placeholder\n",
    "             print(f\"Skipping {exp_name}: Error initializing/testing HierEncoderRNN placeholder. {e}\")\n",
    "             return { 'status': 'skipped', 'reason': f'HierEncoderRNN placeholder error: {e}' }\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown encoder_type: {encoder_type}\")\n",
    "\n",
    "    # Decoder\n",
    "    decoder_hidden_dim = config_params['HIDDEN_DIM'] # Adjust if encoder processing changes hidden dim\n",
    "    if decoder_type == 'basic':\n",
    "        decoder = DecoderRNN(vocab_size, config_params['EMBED_DIM'], decoder_hidden_dim,\n",
    "                           config_params['NUM_LAYERS_DEC'], config_params['DROPOUT']).to(device)\n",
    "    elif decoder_type == 'decoder2':\n",
    "        decoder = Decoder2RNN(vocab_size, config_params['EMBED_DIM'], decoder_hidden_dim,\n",
    "                            config_params['DROPOUT']).to(device)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown decoder_type: {decoder_type}\")\n",
    "\n",
    "    # Seq2Seq Model\n",
    "    model = Seq2seqRNN(encoder, decoder, device, BOS_IDX, EOS_IDX, PAD_IDX).to(device)\n",
    "\n",
    "    # --- Handle Loading/Skipping Training ---\n",
    "    model_save_path = os.path.join(config_params['MODEL_SAVE_DIR'], f\"{exp_name}_model.pt\")\n",
    "    train_time = 0\n",
    "    best_valid_loss = float('inf')\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "\n",
    "\n",
    "    if load_model_path:\n",
    "        print(f\"Loading pre-trained model from: {load_model_path}\")\n",
    "        try:\n",
    "            # Apply the fix here!\n",
    "            model.load_state_dict(torch.load(load_model_path, map_location=device, weights_only=True))\n",
    "            print(\"Model loaded successfully.\")\n",
    "            skip_training = True # Force skip training if loading specific path\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: Model file not found at {load_model_path}. Cannot evaluate.\")\n",
    "            return { 'status': 'error', 'reason': f'Model file not found: {load_model_path}' }\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model state dict from {load_model_path}: {e}\")\n",
    "            return { 'status': 'error', 'reason': f'Error loading state dict: {e}' }\n",
    "\n",
    "    elif skip_training:\n",
    "        print(\"Skipping training as requested.\")\n",
    "        # Attempt to load if a saved model for *this* experiment exists\n",
    "        if os.path.exists(model_save_path):\n",
    "             print(f\"Found existing model file for this config: {model_save_path}. Loading...\")\n",
    "             try:\n",
    "                 # Apply the fix here!\n",
    "                 model.load_state_dict(torch.load(model_save_path, map_location=device, weights_only=True))\n",
    "             except Exception as e:\n",
    "                 print(f\"Warning: Error loading existing model {model_save_path}: {e}. Evaluation might fail.\")\n",
    "        else:\n",
    "             print(f\"Warning: Skipping training, but no model file found at {model_save_path}. Evaluation might fail.\")\n",
    "\n",
    "\n",
    "    # --- Training ---\n",
    "    if not skip_training:\n",
    "        # Load GloVe if needed\n",
    "        if use_glove:\n",
    "            glove_matrix = load_glove_embeddings(config_params['GLOVE_FILE'], word2idx, config_params['EMBED_DIM'])\n",
    "            if glove_matrix is not None:\n",
    "                model.load_embeddings(glove_matrix.to(device), freeze=glove_freeze)\n",
    "            else:\n",
    "                print(\"GloVe embeddings not loaded, continuing with random initialization.\")\n",
    "\n",
    "        # Optimizer and Criterion\n",
    "        optimizer = optim.Adam(model.parameters(), lr=config_params['LEARNING_RATE'])\n",
    "        criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "        print(\"Starting training...\")\n",
    "        train_loop_start = time.time()\n",
    "        for epoch in range(config_params['NUM_EPOCHS']):\n",
    "            epoch_start_time = time.time()\n",
    "\n",
    "            train_loss = train_epoch(model, train_dataloader, optimizer, criterion, config_params['CLIP_GRAD'], device)\n",
    "            valid_loss = evaluate(model, val_dataloader, criterion, device)\n",
    "\n",
    "            train_losses.append(train_loss)\n",
    "            valid_losses.append(valid_loss)\n",
    "\n",
    "            epoch_end_time = time.time()\n",
    "            epoch_mins, epoch_secs = divmod(epoch_end_time - epoch_start_time, 60)\n",
    "\n",
    "            if valid_loss < best_valid_loss:\n",
    "                best_valid_loss = valid_loss\n",
    "                print(f\"Epoch {epoch+1}/{config_params['NUM_EPOCHS']} | Time: {epoch_mins:.0f}m {epoch_secs:.0f}s | Saving Best Model...\")\n",
    "                torch.save(model.state_dict(), model_save_path) # Save the best model`\n",
    "            else:\n",
    "                 print(f\"Epoch {epoch+1}/{config_params['NUM_EPOCHS']} | Time: {epoch_mins:.0f}m {epoch_secs:.0f}s\")\n",
    "\n",
    "            print(f'\\tTrain Loss: {train_loss:.4f} | Val. Loss: {valid_loss:.4f}')\n",
    "\n",
    "        train_time = time.time() - train_loop_start\n",
    "        print(f\"Training finished. Time taken: {train_time:.2f} seconds\")\n",
    "\n",
    "        # Load best model for evaluation\n",
    "        if os.path.exists(model_save_path):\n",
    "            print(f\"Loading best trained model from {model_save_path}\")\n",
    "            try:\n",
    "                # Apply the fix here!\n",
    "                model.load_state_dict(torch.load(model_save_path, map_location=device, weights_only=True))\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Error loading best model state dict: {e}. Evaluating with last epoch's model.\")\n",
    "        else:\n",
    "            print(\"Warning: Best model file not found after training. Evaluating with last epoch's model.\")\n",
    "\n",
    "    # --- Evaluation ---\n",
    "    eval_start_time = time.time()\n",
    "    generation_params = {\n",
    "        'max_len': config_params['MAX_NEW_TOKENS'],\n",
    "        'beam_width': beam_width\n",
    "    }\n",
    "    # Check if beam search is requested and implemented\n",
    "    if beam_width > 1:\n",
    "         try:\n",
    "             # Attempt generation - this will raise NotImplementedError if not implemented\n",
    "             _ = model.generate(torch.randint(0,10,(1,5)).to(device), torch.tensor([5]), max_len=5, beam_width=beam_width)\n",
    "             print(f\"Evaluating with Beam Search (Width={beam_width})...\")\n",
    "         except NotImplementedError as e:\n",
    "              print(f\"Cannot evaluate {exp_name} with Beam Search: Beam search not implemented. {e}\")\n",
    "              return { 'status': 'skipped', 'reason': 'Beam search not implemented' }\n",
    "         except Exception as e: # Catch other potential errors\n",
    "              print(f\"Cannot evaluate {exp_name} with Beam Search: Error during generation call. {e}\")\n",
    "              return { 'status': 'error', 'reason': f'Beam search generation error: {e}' }\n",
    "    else:\n",
    "         print(\"Evaluating with Greedy Search...\")\n",
    "\n",
    "\n",
    "    rouge_scores = predict_and_evaluate_rouge(model, test_dataloader, idx2word, device, generation_params)\n",
    "    eval_time = time.time() - eval_start_time\n",
    "\n",
    "    exp_end_time = time.time()\n",
    "    total_exp_time = exp_end_time - exp_start_time\n",
    "\n",
    "    # --- Store Results ---\n",
    "    results_data = {\n",
    "        'status': 'completed',\n",
    "        'exp_name': exp_name,\n",
    "        'encoder': encoder_type,\n",
    "        'decoder': decoder_type,\n",
    "        'glove': use_glove,\n",
    "        'beam_width': beam_width,\n",
    "        'train_time_s': train_time,\n",
    "        'eval_time_s': eval_time,\n",
    "        'total_time_s': total_exp_time,\n",
    "        'best_val_loss': best_valid_loss if not skip_training else 'N/A',\n",
    "        'rouge1': rouge_scores['rouge1'],\n",
    "        'rouge2': rouge_scores['rouge2'],\n",
    "        'rougeL': rouge_scores['rougeL'],\n",
    "        'train_losses': train_losses, # Store loss history\n",
    "        'valid_losses': valid_losses,\n",
    "        'model_path': model_save_path if not skip_training else load_model_path\n",
    "    }\n",
    "    print(f\"--- Experiment {exp_name} Finished --- Time: {total_exp_time:.2f}s | ROUGE-L: {rouge_scores['rougeL']:.4f}\")\n",
    "    return results_data\n",
    "\n",
    "\n",
    "# --- Main Execution Block ---\n",
    "if __name__ == \"__main__\":\n",
    "    main_start_time = time.time()\n",
    "    print(f\"Main execution started. Using device: {DEVICE}\")\n",
    "    print(\"NOTE: Experiments will run sequentially in this script.\")\n",
    "    print(\"      For parallel execution on Kaggle, create separate notebooks for each part and run them concurrently.\")\n",
    "\n",
    "    # --- Collect All Config Params into a dictionary ---\n",
    "    # Allows passing all hyperparameters easily to the run_experiment function\n",
    "    config_params = {\n",
    "        'TRAIN_PROC_FILE': TRAIN_PROC_FILE,\n",
    "        'VAL_PROC_FILE': VAL_PROC_FILE,\n",
    "        'TEST_PROC_FILE': TEST_PROC_FILE,\n",
    "        'VOCAB_FILE': VOCAB_FILE,\n",
    "        'GLOVE_FILE': GLOVE_FILE,\n",
    "        'MODEL_SAVE_DIR': MODEL_SAVE_DIR,\n",
    "        'VOCAB_FREQ_THRESHOLD_PERCENT': VOCAB_FREQ_THRESHOLD_PERCENT,\n",
    "        'PAD_TOKEN': PAD_TOKEN, 'UNK_TOKEN': UNK_TOKEN, 'BOS_TOKEN': BOS_TOKEN, 'EOS_TOKEN': EOS_TOKEN,\n",
    "        'HIDDEN_DIM': HIDDEN_DIM, 'EMBED_DIM': EMBED_DIM,\n",
    "        'NUM_LAYERS_ENC': NUM_LAYERS_ENC, 'NUM_LAYERS_DEC': NUM_LAYERS_DEC,\n",
    "        'DROPOUT': DROPOUT, 'LEARNING_RATE': LEARNING_RATE,\n",
    "        'BATCH_SIZE': BATCH_SIZE, 'NUM_EPOCHS': NUM_EPOCHS, 'CLIP_GRAD': CLIP_GRAD,\n",
    "        'MAX_NEW_TOKENS': MAX_NEW_TOKENS, 'BEAM_WIDTH': BEAM_WIDTH,\n",
    "        'SEED': SEED, 'DEVICE': DEVICE, 'REPORT_INTERVAL': REPORT_INTERVAL\n",
    "    }\n",
    "    # Ensure model save directory exists\n",
    "    os.makedirs(config_params['MODEL_SAVE_DIR'], exist_ok=True)\n",
    "\n",
    "    # --- 1. Build or Load Vocabulary ---\n",
    "    vocab_start_time = time.time()\n",
    "    word2idx, idx2word = build_or_load_vocab(\n",
    "        config_params['TRAIN_PROC_FILE'],\n",
    "        config_params['VOCAB_FREQ_THRESHOLD_PERCENT'],\n",
    "        save_path=config_params['VOCAB_FILE']\n",
    "    )\n",
    "    print(f\"Time taken for vocabulary: {time.time() - vocab_start_time:.2f} seconds\")\n",
    "\n",
    "    # --- 2. Create Datasets and DataLoaders ---\n",
    "    data_load_start_time = time.time()\n",
    "    # Create datasets (handle potential errors)\n",
    "    try:\n",
    "        train_dataset = WikipediaDataset(config_params['TRAIN_PROC_FILE'], word2idx)\n",
    "        val_dataset = WikipediaDataset(config_params['VAL_PROC_FILE'], word2idx)\n",
    "        test_dataset = WikipediaDataset(config_params['TEST_PROC_FILE'], word2idx)\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: Processed data file not found: {e}. Did Task A run successfully?\")\n",
    "        exit()\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating datasets: {e}\")\n",
    "        exit()\n",
    "\n",
    "    # Create dataloaders\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=config_params['BATCH_SIZE'], shuffle=True, collate_fn=collate_fn, num_workers=2, pin_memory=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=config_params['BATCH_SIZE'], shuffle=False, collate_fn=collate_fn, num_workers=2, pin_memory=True)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=config_params['BATCH_SIZE'], shuffle=False, collate_fn=collate_fn, num_workers=2, pin_memory=True)\n",
    "    print(f\"Time taken for data loading: {time.time() - data_load_start_time:.2f} seconds\")\n",
    "\n",
    "    # --- Run Experiments Sequentially ---\n",
    "    all_results = {}\n",
    "    basic_rnn_model_path = None # To store path for beam search eval\n",
    "\n",
    "    # === Part 1: Basic RNN (Greedy) ===\n",
    "    results = run_experiment(\n",
    "        exp_name='BasicRNN',\n",
    "        encoder_type='basic',\n",
    "        decoder_type='basic',\n",
    "        use_glove=False,\n",
    "        glove_freeze=False, # Not applicable\n",
    "        beam_width=1, # Greedy\n",
    "        word2idx=word2idx, idx2word=idx2word,\n",
    "        train_dataloader=train_dataloader, val_dataloader=val_dataloader, test_dataloader=test_dataloader,\n",
    "        device=DEVICE, config_params=config_params\n",
    "    )\n",
    "    all_results['BasicRNN'] = results\n",
    "    if results.get('status') == 'completed':\n",
    "        basic_rnn_model_path = results.get('model_path') # Save path for later\n",
    "\n",
    "    # === Part 2: RNN + GloVe (Greedy) ===\n",
    "    results = run_experiment(\n",
    "        exp_name='BasicRNN_GloVe',\n",
    "        encoder_type='basic',\n",
    "        decoder_type='basic',\n",
    "        use_glove=True,\n",
    "        glove_freeze=False, # Example: Fine-tune GloVe embeddings\n",
    "        beam_width=1, # Greedy\n",
    "        word2idx=word2idx, idx2word=idx2word,\n",
    "        train_dataloader=train_dataloader, val_dataloader=val_dataloader, test_dataloader=test_dataloader,\n",
    "        device=DEVICE, config_params=config_params\n",
    "    )\n",
    "    all_results['BasicRNN_GloVe'] = results\n",
    "\n",
    "    # === Part 3: RNN + Hierarchical Encoder (Attempt/Skip) ===\n",
    "    results = run_experiment(\n",
    "        exp_name='HierEncRNN',\n",
    "        encoder_type='hierarchical', # This will likely trigger the skip\n",
    "        decoder_type='basic',\n",
    "        use_glove=False,\n",
    "        glove_freeze=False,\n",
    "        beam_width=1,\n",
    "        word2idx=word2idx, idx2word=idx2word,\n",
    "        train_dataloader=train_dataloader, val_dataloader=val_dataloader, test_dataloader=test_dataloader,\n",
    "        device=DEVICE, config_params=config_params\n",
    "    )\n",
    "    all_results['HierEncRNN'] = results\n",
    "\n",
    "    # === Part 4: RNN + Deeper Decoder (Greedy) ===\n",
    "    results = run_experiment(\n",
    "        exp_name='Decoder2RNN',\n",
    "        encoder_type='basic',\n",
    "        decoder_type='decoder2', # Use the deeper decoder\n",
    "        use_glove=False,\n",
    "        glove_freeze=False,\n",
    "        beam_width=1, # Greedy\n",
    "        word2idx=word2idx, idx2word=idx2word,\n",
    "        train_dataloader=train_dataloader, val_dataloader=val_dataloader, test_dataloader=test_dataloader,\n",
    "        device=DEVICE, config_params=config_params\n",
    "    )\n",
    "    all_results['Decoder2RNN'] = results\n",
    "\n",
    "    # === Part 5: Basic RNN + Beam Search ===\n",
    "    if basic_rnn_model_path and os.path.exists(basic_rnn_model_path):\n",
    "        results = run_experiment(\n",
    "            exp_name='BasicRNN_Beam',\n",
    "            encoder_type='basic', # Must match the model being loaded\n",
    "            decoder_type='basic', # Must match the model being loaded\n",
    "            use_glove=False,      # Config doesn't matter when loading state dict directly\n",
    "            glove_freeze=False,\n",
    "            beam_width=config_params['BEAM_WIDTH'], # Use beam search width\n",
    "            word2idx=word2idx, idx2word=idx2word,\n",
    "            train_dataloader=train_dataloader, val_dataloader=val_dataloader, test_dataloader=test_dataloader,\n",
    "            device=DEVICE, config_params=config_params,\n",
    "            skip_training=True,             # Skip training phase\n",
    "            load_model_path=basic_rnn_model_path # Load the previously trained basic model\n",
    "        )\n",
    "        all_results['BasicRNN_Beam'] = results\n",
    "    else:\n",
    "        print(\"\\nSkipping Beam Search evaluation because the BasicRNN model was not trained successfully or path not found.\")\n",
    "        all_results['BasicRNN_Beam'] = {'status': 'skipped', 'reason': 'BasicRNN model path not available'}\n",
    "\n",
    "\n",
    "    # --- Print Summary ---\n",
    "    print(\"\\n\\n\" + \"=\"*50)\n",
    "    print(\" Final Results Summary (Part B)\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    summary_data = []\n",
    "    for name, res in all_results.items():\n",
    "         if res.get('status') == 'completed':\n",
    "             summary_data.append({\n",
    "                 'Experiment': name,\n",
    "                 'Encoder': res.get('encoder', 'N/A'),\n",
    "                 'Decoder': res.get('decoder', 'N/A'),\n",
    "                 'GloVe': res.get('glove', 'N/A'),\n",
    "                 'BeamWidth': res.get('beam_width', 'N/A'),\n",
    "                 'ROUGE-1': res.get('rouge1', 0.0),\n",
    "                 'ROUGE-2': res.get('rouge2', 0.0),\n",
    "                 'ROUGE-L': res.get('rougeL', 0.0),\n",
    "                 'Eval Time (s)': res.get('eval_time_s', 0.0),\n",
    "                 'Train Time (s)': res.get('train_time_s', 0.0),\n",
    "                 'Total Time (s)': res.get('total_time_s', 0.0),\n",
    "             })\n",
    "         else:\n",
    "             summary_data.append({\n",
    "                 'Experiment': name,\n",
    "                 'Status': res.get('status', 'unknown'),\n",
    "                 'Reason': res.get('reason', 'N/A')\n",
    "             })\n",
    "\n",
    "    if summary_data:\n",
    "        summary_df = pd.DataFrame(summary_data)\n",
    "        print(summary_df.to_markdown(index=False, floatfmt=\".4f\"))\n",
    "        # Save summary to CSV\n",
    "        summary_df.to_csv(os.path.join(config_params['MODEL_SAVE_DIR'], 'taskB_results_summary.csv'), index=False)\n",
    "    else:\n",
    "        print(\"No experiments completed successfully.\")\n",
    "\n",
    "\n",
    "    main_end_time = time.time()\n",
    "    total_script_time = main_end_time - main_start_time\n",
    "    print(f\"\\nTotal script execution time: {total_script_time:.2f} seconds ({total_script_time/60:.2f} minutes)\")\n",
    "    print(\"--- Task B Complete ---\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 870709,
     "sourceId": 1483651,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7129969,
     "sourceId": 11386325,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

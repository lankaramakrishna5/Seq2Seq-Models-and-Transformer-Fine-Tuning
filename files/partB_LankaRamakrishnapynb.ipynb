{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gP-S2KiqNPTA"
   },
   "source": [
    "**IMPORTING LIBRARIES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bHf51wWgIKT7"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gP-S2KiqNPTA"
   },
   "source": [
    "**DATA LOADING AND PREPROCESSING FUNCTION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D82qVX73IM9t",
    "outputId": "af38fc25-8f58-49b7-99fc-4d5fd370057d"
   },
   "outputs": [],
   "source": [
    "# Define Data Loading and Preprocessing Function\n",
    "def load_data(filepath):\n",
    "    ### START CODE HERE ###\n",
    "    df = pd.read_csv(filepath)\n",
    "\n",
    "    # Families mapping\n",
    "    families_mapping = {\n",
    "        'Bufonidae': 1,\n",
    "        'Dendrobatidae': 2,\n",
    "        'Hylidae': 3,\n",
    "        'Leptodactylidae': 4\n",
    "    }\n",
    "    df.replace(families_mapping, inplace=True)\n",
    "\n",
    "    # Genus mapping\n",
    "    genus_mapping = {\n",
    "        'Adenomera': 1,\n",
    "        'Ameerega': 2,\n",
    "        'Dendropsophus': 3,\n",
    "        'Hypsiboas': 4,\n",
    "        'Leptodactylus': 5,\n",
    "        'Osteocephalus': 6,\n",
    "        'Rhinella': 7,\n",
    "        'Scinax': 8\n",
    "    }\n",
    "    df.replace(genus_mapping, inplace=True)\n",
    "\n",
    "    # Species mapping\n",
    "    species_mapping = {\n",
    "        'AdenomeraAndre': 1,\n",
    "        'AdenomeraHylaedactylus': 2,\n",
    "        'Ameeregatrivittata': 3,\n",
    "        'HylaMinuta': 4,\n",
    "        'HypsiboasCinerascens': 5,\n",
    "        'HypsiboasCordobae': 6,\n",
    "        'LeptodactylusFuscus': 7,\n",
    "        'OsteocephalusOophagus': 8,\n",
    "        'Rhinellagranulosa': 9,\n",
    "        'ScinaxRuber': 10\n",
    "    }\n",
    "    df.replace(species_mapping, inplace=True)\n",
    "\n",
    "    # Check for missing values\n",
    "    print(\"\\nMissing values \")\n",
    "    print(df.isnull().sum())\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gP-S2KiqNPTA"
   },
   "source": [
    "**PLOTIING FEATURE DISTRIBUTIONS AND BOX PLOTS FOR OUTLIER DETECTION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "def visualize_data(df):\n",
    "    # Get the numeric columns for plotting\n",
    "    numeric_columns = df.select_dtypes(include=[float, int]).columns\n",
    "    num_features = len(numeric_columns)\n",
    "    \n",
    "    # Determine number of rows needed for subplots (4 per row)\n",
    "    num_rows = int(np.ceil(num_features / 4))\n",
    "\n",
    "    # Visualize feature distributions in subplots\n",
    "    if num_features > 0:\n",
    "        fig, axes = plt.subplots(num_rows, 4, figsize=(20, 5 * num_rows))\n",
    "        axes = axes.flatten()  # Flatten the 2D array of axes\n",
    "\n",
    "        for ax, column in zip(axes, numeric_columns):\n",
    "            sns.histplot(df[column], kde=True, ax=ax)\n",
    "            ax.set_title(f'Distribution of {column}')\n",
    "            ax.set_xlabel(column)\n",
    "            ax.set_ylabel('Frequency')\n",
    "\n",
    "        # Hide any unused subplots\n",
    "        for i in range(num_features, len(axes)):\n",
    "            axes[i].set_visible(False)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # Outlier detection using box plots in subplots\n",
    "    if num_features > 0:\n",
    "        fig, axes = plt.subplots(num_rows, 4, figsize=(20, 5 * num_rows))\n",
    "        axes = axes.flatten()  # Flatten the 2D array of axes\n",
    "\n",
    "        for ax, column in zip(axes, numeric_columns):\n",
    "            sns.boxplot(x=df[column], ax=ax)\n",
    "            ax.set_title(f'Outliers in {column}')\n",
    "        \n",
    "        # Hide any unused subplots\n",
    "        for i in range(num_features, len(axes)):\n",
    "            axes[i].set_visible(False)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Example usage:\n",
    "# visualize_data(X1)  # Replace X1 with your DataFrame variable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gP-S2KiqNPTA"
   },
   "source": [
    "**VISUALSISING CORRELATION MATRIX**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Matrix Visualization\n",
    "def plot_correlation_matrix(df):\n",
    "    # Calculate the correlation matrix\n",
    "    correlation_matrix = df.corr()\n",
    "\n",
    "    # Set up the matplotlib figure\n",
    "    plt.figure(figsize=(12, 10))\n",
    "\n",
    "    # Create a heatmap using seaborn\n",
    "    sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap='coolwarm', square=True, cbar_kws={\"shrink\": .8})\n",
    "    plt.title('Correlation Matrix')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gP-S2KiqNPTA"
   },
   "source": [
    "**REMOVING OUTLIERS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Outliers\n",
    "def remove_outliers(df):\n",
    "    # Creating a copy of the DataFrame to avoid modifying the original\n",
    "    df_cleaned = df.copy()\n",
    "\n",
    "    # Iterate over numeric columns\n",
    "    for column in df_cleaned.select_dtypes(include=[float, int]).columns:\n",
    "        # Calculate Q1 (25th percentile) and Q3 (75th percentile)\n",
    "        Q1 = df_cleaned[column].quantile(0.25)\n",
    "        Q3 = df_cleaned[column].quantile(0.75)\n",
    "        IQR = Q3 - Q1  # Interquartile range\n",
    "\n",
    "        # Define bounds for outliers\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "        # Remove outliers\n",
    "        df_cleaned = df_cleaned[(df_cleaned[column] >= lower_bound) & (df_cleaned[column] <= upper_bound)]\n",
    "\n",
    "    return df_cleaned\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gP-S2KiqNPTA"
   },
   "source": [
    "**EXECUTION OF FUNCTIONS AND REMOVING FEATURES WITH HIGH CORRELATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Main Execution Cell\n",
    "filepath = 'Frogs_MFCCs.csv'  # Set the file path\n",
    "df = load_data(filepath)        # Load the data\n",
    "visualize_data(df)              # Visualize distributions and outliers\n",
    "\n",
    "print(\"Data shape before removing outliers:\", df.shape)  # Print shape after removing outliers\n",
    "df_cleaned = remove_outliers(df)  # Remove outliers from the dataset\n",
    "print(\"Data shape after removing outliers:\", df_cleaned.shape)  # Print shape after removing outliers\n",
    "\n",
    "plot_correlation_matrix(df_cleaned)  # Plot the correlation matrix for cleaned data\n",
    "\n",
    "# Assuming df_cleaned is your DataFrame\n",
    "# Step 1: Calculate the correlation matrix\n",
    "correlation_matrix = df_cleaned.corr()\n",
    "\n",
    "# Step 2: Find pairs of columns with a correlation greater than the threshold\n",
    "threshold = 0.95\n",
    "to_drop = set()\n",
    "\n",
    "# Iterate through the correlation matrix to identify columns to drop\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > threshold:  # Check correlation\n",
    "            colname = correlation_matrix.columns[i]  # Name of the column to drop\n",
    "            to_drop.add(colname)\n",
    "\n",
    "# Step 3: Drop the identified columns from the DataFrame\n",
    "df_cleaned_reduced = df_cleaned.drop(columns=to_drop)\n",
    "\n",
    "# Display the original and reduced DataFrame shapes\n",
    "print(\"Original DataFrame shape:\", df_cleaned.shape)\n",
    "print(\"Reduced DataFrame shape:\", df_cleaned_reduced.shape)\n",
    "\n",
    "df_cleaned=df_cleaned_reduced\\\n",
    "\n",
    "plot_correlation_matrix(df_cleaned)  # Plot the correlation matrix for cleaned data\n",
    "\n",
    "X = df_cleaned.values               # Convert cleaned DataFrame to NumPy array\n",
    "print(type(X))                       # Print type of X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature Engineering :**\n",
    "Polynomial features transform existing features by raising them to different powers (e.g., squaring, cubing) or creating combinations of features to capture nonlinear relationships. By adding polynomial features, you can expand the feature space, potentially improving a model's ability to distinguish clusters that have complex boundaries.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Assume X is your NumPy array containing MFCC features\n",
    "\n",
    "# Step 1: Generate polynomial features up to degree 2\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_poly = poly.fit_transform(X)  # Apply polynomial features to the NumPy array\n",
    "\n",
    "# Create a placeholder target variable for RFE\n",
    "# For demonstration, we'll use the mean of each row as a pseudo target\n",
    "y = np.mean(X, axis=1)  # Generates a target based on the mean of each row\n",
    "\n",
    "# Step 2: Using RFE with a Linear Regression model\n",
    "model = LinearRegression()\n",
    "selector = RFE(estimator=model, n_features_to_select=25)\n",
    "selector = selector.fit(X_poly, y)\n",
    "\n",
    "# Get the mask of selected features\n",
    "selected_indices = selector.support_\n",
    "\n",
    "# Get selected features based on the mask\n",
    "X_selected = X_poly[:, selected_indices]\n",
    "\n",
    "# Display the shape of the selected features\n",
    "print(\"Shape of the selected features (top 25):\", X_selected.shape)\n",
    "\n",
    "# If you want to view the selected polynomial features as a DataFrame\n",
    "df_X_poly = pd.DataFrame(X_selected)\n",
    "X=df_X_poly\n",
    "print(\"Selected Polynomial Features (first few rows):\")\n",
    "print(df_X_poly.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing K-Means Initialization Methods (k = 3)\n",
    "\n",
    "This code compares the **inertia** (sum of squared distances to cluster centers) for K-Means clustering with `random` and `k-means++` initializations using **3 clusters** over **10 iterations**. Lower inertia values indicate more compact clusters.\n",
    "\n",
    "The plot below shows inertia for each initialization method across the 10 iterations, allowing for a direct comparison of their effectiveness. Average inertia for each method is displayed for additional insight.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of clusters\n",
    "k = 3\n",
    "\n",
    "# Initialize lists to store inertia (sum of squared distances) values\n",
    "inertia_random = []\n",
    "inertia_kmeanspp = []\n",
    "\n",
    "# Run K-Means with different initializations for a fixed range of cluster trials\n",
    "n_runs = 50\n",
    "for i in range(n_runs):\n",
    "    # Random initialization\n",
    "    kmeans_random = KMeans(n_clusters=k, init='random', random_state=i)\n",
    "    kmeans_random.fit(X)\n",
    "    inertia_random.append(kmeans_random.inertia_)\n",
    "\n",
    "    # k-means++ initialization\n",
    "    kmeans_kpp = KMeans(n_clusters=k, init='k-means++', random_state=i)\n",
    "    kmeans_kpp.fit(X)\n",
    "    inertia_kmeanspp.append(kmeans_kpp.inertia_)\n",
    "\n",
    "# Plot the inertia values for each initialization method\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, n_runs + 1), inertia_random, marker='o', linestyle='-', label='Random Initialization')\n",
    "plt.plot(range(1, n_runs + 1), inertia_kmeanspp, marker='s', linestyle='--', label='k-means++ Initialization')\n",
    "plt.xlabel(\"Run\")\n",
    "plt.ylabel(\"Sum of Squared Distances (Inertia)\")\n",
    "plt.title(\"Inertia Comparison: Random vs k-means++ Initialization\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Display average inertia for each method\n",
    "print(\"Average inertia with Random Initialization:\", np.mean(inertia_random))\n",
    "print(\"Average inertia with k-means++ Initialization:\", np.mean(inertia_kmeanspp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MFCC Feature Contributions to Cluster Separation\n",
    "\n",
    "This analysis identifies which MFCC features contribute most to cluster separation by calculating the variance of each feature across clusters. The bar plot below shows MFCCs with higher variance, indicating stronger contributions to defining distinct clusters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assume you already have a dataset X with MFCC features\n",
    "\n",
    "# Step 1: Run K-Means clustering\n",
    "k = 3  # Define number of clusters\n",
    "kmeans = KMeans(n_clusters=k, random_state=0)\n",
    "clusters = kmeans.fit_predict(X)\n",
    "\n",
    "# Step 2: Calculate variance of each MFCC feature across clusters\n",
    "cluster_centers = pd.DataFrame(kmeans.cluster_centers_, columns=X.columns)\n",
    "feature_variances = cluster_centers.var(axis=0)\n",
    "\n",
    "# Step 3: Visualize feature contributions\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(X.columns, feature_variances)\n",
    "plt.xlabel(\"MFCC Features\")\n",
    "plt.ylabel(\"Variance Across Clusters\")\n",
    "plt.title(\"MFCC Feature Contributions to Cluster Separation\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gP-S2KiqNPTA"
   },
   "source": [
    "**PLOT FOR ELBOW METHOD**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 508
    },
    "id": "4fGqE_BjIhUI",
    "outputId": "8c5c080a-0a65-404d-c150-f6764922f608"
   },
   "outputs": [],
   "source": [
    "# List to store the inertia values\n",
    "inertia = []\n",
    "\n",
    "# Range of clusters to test\n",
    "cluster_range = range(1, 11)\n",
    "\n",
    "# Apply K-means for each number of clusters\n",
    "\n",
    "for k in cluster_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=0)\n",
    "    kmeans.fit(X)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "# Plotting the elbow method\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(cluster_range, inertia, marker='o')\n",
    "plt.title('Elbow Method for Optimal k')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Sum of Squared Distances (SSD)')\n",
    "plt.xticks(cluster_range)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tKt8eaAtLVxM"
   },
   "source": [
    "**SILHOUTTE SCORE FOR EVALUATION:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DjiCUzGqLcS3"
   },
   "outputs": [],
   "source": [
    "range_n_clusters = range(2, 11)\n",
    "silhouette_avg = []\n",
    "\n",
    "for n_clusters in range_n_clusters:\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "    cluster_labels = kmeans.fit_predict(X)\n",
    "\n",
    "    # Calculate the silhouette score\n",
    "    silhouette_avg.append(silhouette_score(X, cluster_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 498
    },
    "id": "G1bU1060LxLn",
    "outputId": "4fc8116f-d019-4238-c58f-52e7d0d37a60"
   },
   "outputs": [],
   "source": [
    "# Plotting the silhouette scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range_n_clusters, silhouette_avg, marker='o')\n",
    "plt.title('Silhouette Score vs Number of Clusters')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Average Silhouette Score')\n",
    "plt.xticks(range_n_clusters)\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2gUIoYdCL4BK",
    "outputId": "a036542f-a9d0-4eaa-e324-6a6768e998ac"
   },
   "outputs": [],
   "source": [
    "# Print silhouette scores for each number of clusters\n",
    "m = -1 ;\n",
    "for n_clusters, score in zip(range_n_clusters, silhouette_avg):\n",
    "    print(f'Number of clusters: {n_clusters}, Silhouette Score: {score:.4f}')\n",
    "    if score > m :\n",
    "        m = score\n",
    "        k = n_clusters\n",
    "print(f'Number of clusters for maximum Silhouette Score: {k}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gP-S2KiqNPTA"
   },
   "source": [
    "**DAVIES BOULDIN INDEX**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bv1t7nZYNT87",
    "outputId": "311a0781-781d-4bfe-d03a-82920e2d8787"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import davies_bouldin_score\n",
    "range_n_clusters = range(2, 11)\n",
    "db_index = []\n",
    "min = 2 ;\n",
    "for n_clusters in range_n_clusters:\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "    cluster_labels = kmeans.fit_predict(X)\n",
    "\n",
    "    # Calculate the Davies-Bouldin Index\n",
    "    db_index.append(davies_bouldin_score(X, cluster_labels))\n",
    "    if min < db_index[-1] :\n",
    "        min = db_index[-1]\n",
    "        k = n_clusters\n",
    "print(f'Number of clusters for minimum Davies-Bouldin Index: {k}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 498
    },
    "id": "XaTfIkNwOCro",
    "outputId": "9fab014f-1123-4952-8ecb-a3de96f5a1dc"
   },
   "outputs": [],
   "source": [
    "# Plotting the Davies-Bouldin Index\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range_n_clusters, db_index, marker='o')\n",
    "plt.title('Davies-Bouldin Index vs Number of Clusters')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Davies-Bouldin Index')\n",
    "plt.xticks(range_n_clusters)\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tlCREebXOF4F"
   },
   "outputs": [],
   "source": [
    "# Print DBI for each number of clusters\n",
    "for n_clusters, db in zip(range_n_clusters, db_index):\n",
    "    print(f'Number of clusters: {n_clusters}, Davies-Bouldin Index: {db:.4f}')\n",
    "print(f'Number of clusters for minimum Davies-Bouldin Index: {k}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gP-S2KiqNPTA"
   },
   "source": [
    "**CALSINKI HARABASZ INDEX**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KatlszoiOnk7"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import calinski_harabasz_score\n",
    "ch_index = []\n",
    "for n_clusters in range_n_clusters:\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "    cluster_labels = kmeans.fit_predict(X)\n",
    "\n",
    "    # Calculate the Calinski-Harabasz Index\n",
    "    ch_index.append(calinski_harabasz_score(X, cluster_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 495
    },
    "id": "piym6kmSPHFH",
    "outputId": "72394926-b14b-432d-d924-e7f63b46382d"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range_n_clusters, ch_index, marker='o')\n",
    "plt.title('Calinski-Harabasz Index vs Number of Clusters')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Calinski-Harabasz Index')\n",
    "plt.xticks(range_n_clusters)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LdnPBvvtPHti",
    "outputId": "2a7b5275-b4ca-40fc-824f-76e070006131"
   },
   "outputs": [],
   "source": [
    "max = 0\n",
    "for n_clusters, ch in zip(range_n_clusters, ch_index):\n",
    "    print(f'Number of clusters: {n_clusters}, Calinski-Harabasz Index: {ch:.4f}')\n",
    "    if max < ch :\n",
    "        max = ch\n",
    "        k = n_clusters\n",
    "print(f'Number of clusters for maximum Calinski-Harabasz Index: {k}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nibZMeJkPzAD"
   },
   "source": [
    "**PCA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "XfZyqpoqP1D5",
    "outputId": "f2f5b46b-404a-478c-cfdc-9f5ef6c44032"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Compute the covariance matrix\n",
    "cov_matrix = np.cov(X_scaled, rowvar=False)\n",
    "\n",
    "# Compute eigenvalues and eigenvectors\n",
    "eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)\n",
    "\n",
    "# Sort eigenvalues and corresponding eigenvectors\n",
    "sorted_indices = np.argsort(eigenvalues)[::-1]\n",
    "eigenvalues_sorted = eigenvalues[sorted_indices]\n",
    "eigenvectors_sorted = eigenvectors[:, sorted_indices]\n",
    "\n",
    "# Select the top k eigenvectors (principal components)\n",
    "k = 2  # Number of dimensions to reduce to\n",
    "W = eigenvectors_sorted[:, :k]\n",
    "\n",
    "# Project the data onto the new feature space\n",
    "X_pca = X_scaled.dot(W)\n",
    "\n",
    "# Apply K-means clustering\n",
    "n_clusters_values = [2,3]  # Set the number of clusters\n",
    "for n_clusters in n_clusters_values:\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "    cluster_labels = kmeans.fit_predict(X_pca)\n",
    "\n",
    "    # Plot the PCA results with clusters\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=cluster_labels, cmap='viridis', alpha=0.7)\n",
    "    plt.title(f'PCA with K-means Clustering(K ={n_clusters})')\n",
    "    plt.xlabel('Principal Component 1')\n",
    "    plt.ylabel('Principal Component 2')\n",
    "    plt.colorbar(scatter, label='Cluster Label')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ki-jx4SSxI0"
   },
   "source": [
    "**Agglomerative Hierarchical Clustering or DBSCAN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 571
    },
    "id": "VCjd8xFVSwfs",
    "outputId": "b787e23e-f3f0-4471-e6f7-95dd60193192"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "# Apply DBSCAN\n",
    "dbscan = DBSCAN(eps=0.5, min_samples=5)  # Adjust eps and min_samples as needed\n",
    "cluster_labels = dbscan.fit_predict(X_pca)\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "# Use a scatter plot where the color corresponds to the cluster labels\n",
    "scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=cluster_labels, cmap='viridis', alpha=0.7)\n",
    "plt.title('DBSCAN Clustering Result')\n",
    "plt.xlabel('Feature 1 (standardized)')\n",
    "plt.ylabel('Feature 2 (standardized)')\n",
    "plt.colorbar(scatter, label='Cluster Label')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Print the number of clusters found\n",
    "n_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)  # Exclude noise points\n",
    "n_noise = list(cluster_labels).count(-1)\n",
    "print(f'Estimated number of clusters: {n_clusters}')\n",
    "print(f'Estimated number of noise points: {n_noise}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
